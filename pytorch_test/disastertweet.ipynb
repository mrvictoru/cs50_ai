{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this notebook will do sentiment analysis on the disaster tweets from kaggle dataset using hugging face library\n",
    "\n",
    "#importing libraries\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create custom dataset \n",
    "\n",
    "class disaster_tweets(Dataset):\n",
    "    def __init__(self, train_text_list, labels, tokenizer):\n",
    "        self.encoding = tokenizer(train_text_list, padding=True, truncation=True)\n",
    "        self.labels = labels\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return dictionary compatible with the huggingface transformers\n",
    "        item = {key:torch.tensor(val[idx]) for key, val in self.encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from data/tweetclassified/train.csv and seperate into train and eval\n",
    "train_csv_path = 'data/tweetclassified/train.csv'\n",
    "df = pd.read_csv(train_csv_path)\n",
    "df['text'] = df['text'].replace(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)',\"\",regex=True)\n",
    "tweet = df['text'].tolist()\n",
    "label = df['target'].tolist()\n",
    "\n",
    "train_text, val_text, train_label, val_label = train_test_split(tweet, label, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = disaster_tweets(train_text, train_label, tokenizer)\n",
    "val_dataset = disaster_tweets(val_text, val_label, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "#setting up training process using hugging face's Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./diastertweetresult',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./diastertweetresult/logs',\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6090\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1524\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b08b8ae50204f3fb7ca9c1db4cb18bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1087, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.05}\n",
      "{'loss': 1.9917, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 1.6015, 'learning_rate': 6e-06, 'epoch': 0.16}\n",
      "{'loss': 1.2251, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.21}\n",
      "{'loss': 0.8071, 'learning_rate': 1e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6759, 'learning_rate': 1.2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5559, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.37}\n",
      "{'loss': 0.5154, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4587, 'learning_rate': 1.8e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4828, 'learning_rate': 2e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3771, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 0.4253, 'learning_rate': 2.4e-05, 'epoch': 0.63}\n",
      "{'loss': 0.4662, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5023, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.73}\n",
      "{'loss': 0.4406, 'learning_rate': 3e-05, 'epoch': 0.79}\n",
      "{'loss': 0.4411, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.84}\n",
      "{'loss': 0.4269, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4554, 'learning_rate': 3.6e-05, 'epoch': 0.94}\n",
      "{'loss': 0.4222, 'learning_rate': 3.8e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3196, 'learning_rate': 4e-05, 'epoch': 1.05}\n",
      "{'loss': 0.3844, 'learning_rate': 4.2e-05, 'epoch': 1.1}\n",
      "{'loss': 0.3555, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.15}\n",
      "{'loss': 0.4217, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.21}\n",
      "{'loss': 0.3687, 'learning_rate': 4.8e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./diastertweetresult/checkpoint-500\n",
      "Configuration saved in ./diastertweetresult/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3781, 'learning_rate': 5e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./diastertweetresult/checkpoint-500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3536, 'learning_rate': 4.90234375e-05, 'epoch': 1.36}\n",
      "{'loss': 0.3886, 'learning_rate': 4.8046875e-05, 'epoch': 1.42}\n",
      "{'loss': 0.3769, 'learning_rate': 4.70703125e-05, 'epoch': 1.47}\n",
      "{'loss': 0.3947, 'learning_rate': 4.609375e-05, 'epoch': 1.52}\n",
      "{'loss': 0.3543, 'learning_rate': 4.5117187500000005e-05, 'epoch': 1.57}\n",
      "{'loss': 0.3903, 'learning_rate': 4.4140625000000004e-05, 'epoch': 1.63}\n",
      "{'loss': 0.4377, 'learning_rate': 4.31640625e-05, 'epoch': 1.68}\n",
      "{'loss': 0.351, 'learning_rate': 4.21875e-05, 'epoch': 1.73}\n",
      "{'loss': 0.3322, 'learning_rate': 4.12109375e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4187, 'learning_rate': 4.0234375e-05, 'epoch': 1.84}\n",
      "{'loss': 0.3605, 'learning_rate': 3.92578125e-05, 'epoch': 1.89}\n",
      "{'loss': 0.3131, 'learning_rate': 3.828125e-05, 'epoch': 1.94}\n",
      "{'loss': 0.3695, 'learning_rate': 3.7304687500000005e-05, 'epoch': 1.99}\n",
      "{'loss': 0.2839, 'learning_rate': 3.6328125000000004e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1773, 'learning_rate': 3.53515625e-05, 'epoch': 2.1}\n",
      "{'loss': 0.2393, 'learning_rate': 3.4375e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1904, 'learning_rate': 3.33984375e-05, 'epoch': 2.2}\n",
      "{'loss': 0.2247, 'learning_rate': 3.2421875e-05, 'epoch': 2.26}\n",
      "{'loss': 0.2044, 'learning_rate': 3.14453125e-05, 'epoch': 2.31}\n",
      "{'loss': 0.1801, 'learning_rate': 3.0468750000000002e-05, 'epoch': 2.36}\n",
      "{'loss': 0.2658, 'learning_rate': 2.94921875e-05, 'epoch': 2.41}\n",
      "{'loss': 0.2076, 'learning_rate': 2.8515625e-05, 'epoch': 2.47}\n",
      "{'loss': 0.1417, 'learning_rate': 2.7539062500000003e-05, 'epoch': 2.52}\n",
      "{'loss': 0.2003, 'learning_rate': 2.6562500000000002e-05, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./diastertweetresult/checkpoint-1000\n",
      "Configuration saved in ./diastertweetresult/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1677, 'learning_rate': 2.55859375e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./diastertweetresult/checkpoint-1000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2328, 'learning_rate': 2.4609375e-05, 'epoch': 2.68}\n",
      "{'loss': 0.2286, 'learning_rate': 2.3632812500000003e-05, 'epoch': 2.73}\n",
      "{'loss': 0.2606, 'learning_rate': 2.2656250000000002e-05, 'epoch': 2.78}\n",
      "{'loss': 0.284, 'learning_rate': 2.16796875e-05, 'epoch': 2.83}\n",
      "{'loss': 0.2436, 'learning_rate': 2.0703125e-05, 'epoch': 2.89}\n",
      "{'loss': 0.1426, 'learning_rate': 1.9726562500000003e-05, 'epoch': 2.94}\n",
      "{'loss': 0.1975, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.99}\n",
      "{'loss': 0.116, 'learning_rate': 1.77734375e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0736, 'learning_rate': 1.6796875e-05, 'epoch': 3.1}\n",
      "{'loss': 0.0992, 'learning_rate': 1.58203125e-05, 'epoch': 3.15}\n",
      "{'loss': 0.0949, 'learning_rate': 1.484375e-05, 'epoch': 3.2}\n",
      "{'loss': 0.1145, 'learning_rate': 1.38671875e-05, 'epoch': 3.25}\n",
      "{'loss': 0.1604, 'learning_rate': 1.2890625e-05, 'epoch': 3.31}\n",
      "{'loss': 0.0764, 'learning_rate': 1.19140625e-05, 'epoch': 3.36}\n",
      "{'loss': 0.0909, 'learning_rate': 1.09375e-05, 'epoch': 3.41}\n",
      "{'loss': 0.1121, 'learning_rate': 9.9609375e-06, 'epoch': 3.46}\n",
      "{'loss': 0.1012, 'learning_rate': 8.984375e-06, 'epoch': 3.52}\n",
      "{'loss': 0.0766, 'learning_rate': 8.0078125e-06, 'epoch': 3.57}\n",
      "{'loss': 0.098, 'learning_rate': 7.031250000000001e-06, 'epoch': 3.62}\n",
      "{'loss': 0.1846, 'learning_rate': 6.054687500000001e-06, 'epoch': 3.67}\n",
      "{'loss': 0.1185, 'learning_rate': 5.078125000000001e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0871, 'learning_rate': 4.101562500000001e-06, 'epoch': 3.78}\n",
      "{'loss': 0.1444, 'learning_rate': 3.125e-06, 'epoch': 3.83}\n",
      "{'loss': 0.1322, 'learning_rate': 2.1484375e-06, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./diastertweetresult/checkpoint-1500\n",
      "Configuration saved in ./diastertweetresult/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0876, 'learning_rate': 1.1718750000000001e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./diastertweetresult/checkpoint-1500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1126, 'learning_rate': 1.953125e-07, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.952, 'train_samples_per_second': 163.543, 'train_steps_per_second': 10.231, 'train_loss': 0.3624864375888519, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1524, training_loss=0.3624864375888519, metrics={'train_runtime': 148.952, 'train_samples_per_second': 163.543, 'train_steps_per_second': 10.231, 'train_loss': 0.3624864375888519, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#setting up device to run on cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#load some test data and run prediction\n",
    "test_csv_path = 'data/tweetclassified/test.csv'\n",
    "df = pd.read_csv(test_csv_path)\n",
    "df['text'] = df['text'].replace(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)',\"\",regex=True)\n",
    "tweet = df['text'].tolist()\n",
    "batch = tokenizer(tweet[:10], padding=True, truncation=True, return_tensors='pt')\n",
    "batch.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "648d1dcbd6682217e1a7c0b1a7c0c54c0b39a029f0fde86a6625ffa444d0c385"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('testpython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
