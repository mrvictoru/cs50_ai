{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This notebook uses Long Short-Term Memory (LSTM) on multivariate time series data to predict the closing \n",
    "stock price of a corporation using the past 60 day stock movement.\n",
    "'''\n",
    "# Import the libraries\n",
    "import math\n",
    "import pandas_datareader as web\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Get cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the past stock price\n",
    "# today's date\n",
    "today = pd.to_datetime('today').strftime('%Y-%m-%d')\n",
    "# 5 year ago\n",
    "start = pd.to_datetime('today') - pd.DateOffset(years=5)\n",
    "df = web.DataReader('AAPL', data_source='yahoo', start=start, end=today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "target_data = \"Close\"\n",
    "feature = list(df.columns.difference([target_data]))\n",
    "\n",
    "forecast_lead = 1\n",
    "target = f\"{target_data}_t+{forecast_lead}\"\n",
    "\n",
    "df[target] = df[target_data].shift(-forecast_lead)\n",
    "df = df.iloc[:-forecast_lead]\n",
    "\n",
    "# split data\n",
    "train_size = int(len(df) * 0.8)\n",
    "test_size = len(df) - train_size\n",
    "train, test = df.iloc[0:train_size].copy(), df.iloc[train_size:len(df)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "target_mean = train[target].mean()\n",
    "target_std = train[target].std()\n",
    "\n",
    "for c in train.columns:\n",
    "    mean = train[c].mean()\n",
    "    std = train[c].std()\n",
    "\n",
    "    train[c] = (train[c] - mean) / std\n",
    "    test[c] = (test[c] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, target, features, seq_len = 60):\n",
    "        self.dataframe = dataframe\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "        self.x = torch.tensor(dataframe[features].values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(dataframe[target].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.seq_len - 1:\n",
    "            idx_start = idx - self.seq_len + 1\n",
    "            x = self.x[idx_start:idx+1,:]\n",
    "        else:\n",
    "            padding = self.x[0].repeat(self.seq_len - idx - 1, 1)\n",
    "            x = self.x[0:(idx+1),:]\n",
    "            x = torch.cat((padding, x), dim=0)\n",
    "\n",
    "        return x, self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape:  torch.Size([4, 60, 5])\n",
      "Target shape:  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# create dataset and data loader\n",
    "batch_size = 4\n",
    "seq_len = 60\n",
    "\n",
    "torch.manual_seed(seq_len*batch_size/2)\n",
    "\n",
    "train_dataset = SequenceDataset(train, target, feature, seq_len)\n",
    "test_dataset = SequenceDataset(test, target, feature, seq_len)\n",
    "train_looader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# get the first batch and its shape\n",
    "x, y = next(iter(train_looader))\n",
    "print(\"Features shape: \", x.shape)\n",
    "print(\"Target shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self,n_features, n_hidden, n_layers=1):\n",
    "        super(ShallowRegressionLSTM, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = n_features, \n",
    "            hidden_size=n_hidden, \n",
    "            num_layers=n_layers, \n",
    "            batch_first=True)\n",
    "        self.linear = nn.Linear(in_features = self.n_hidden, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.n_hidden).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.n_hidden).requires_grad_().to(device)\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(hn[0]).flatten() #first dim of hn is the layer dimension\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model, loss and optimizer\n",
    "learning_rate = 5e-5\n",
    "num_hidden = 16\n",
    "\n",
    "model = ShallowRegressionLSTM(n_features = len(feature), n_hidden = num_hidden).to(device)\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function and testing function\n",
    "def train(model, loader, loss, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        batch_loss = loss(y_pred, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "    avg_loss = train_loss / len(loader)\n",
    "    print(f\"Train loss: {avg_loss:.4f}\")\n",
    "    \n",
    "def test(model, loader, loss, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            batch_loss = loss(y_pred, y)\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "    avg_loss = test_loss / len(loader)\n",
    "    print(f\"Test loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "----------------\n",
      "Test loss: 6.5109\n",
      "\n",
      "Training\n",
      "----------------\n",
      "Epoch 1\n",
      "\n",
      "Train loss: 0.9636\n",
      "Test loss: 5.6321\n",
      "Epoch 2\n",
      "\n",
      "Train loss: 0.8094\n",
      "Test loss: 4.7004\n",
      "Epoch 3\n",
      "\n",
      "Train loss: 0.6256\n",
      "Test loss: 3.6037\n",
      "Epoch 4\n",
      "\n",
      "Train loss: 0.4267\n",
      "Test loss: 2.6357\n",
      "Epoch 5\n",
      "\n",
      "Train loss: 0.2682\n",
      "Test loss: 1.9482\n",
      "Epoch 6\n",
      "\n",
      "Train loss: 0.1660\n",
      "Test loss: 1.5050\n",
      "Epoch 7\n",
      "\n",
      "Train loss: 0.1046\n",
      "Test loss: 1.2088\n",
      "Epoch 8\n",
      "\n",
      "Train loss: 0.0712\n",
      "Test loss: 1.0095\n",
      "Epoch 9\n",
      "\n",
      "Train loss: 0.0518\n",
      "Test loss: 0.8719\n",
      "Epoch 10\n",
      "\n",
      "Train loss: 0.0422\n",
      "Test loss: 0.7728\n"
     ]
    }
   ],
   "source": [
    "# train and test model\n",
    "print(\"Untrained test\\n----------------\")\n",
    "test(model, test_loader, loss, device)\n",
    "print(\"\\nTraining\\n----------------\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}\\n\")\n",
    "    train(model, train_looader, loss, optimizer, device)\n",
    "    test(model, test_loader, loss, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('testpython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "648d1dcbd6682217e1a7c0b1a7c0c54c0b39a029f0fde86a6625ffa444d0c385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
