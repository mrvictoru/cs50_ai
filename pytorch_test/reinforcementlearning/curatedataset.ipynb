{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for creating and testing method on curating datasets on stock trading data for offline reinforcement learning with decision transformer model\n",
    "# This will get stock data from yahoo finance\n",
    "# Then it will use the stock data to create gym environments and sample state, action, reward (both randomly or/and by a trained agent ) which then store as a replay buffer\n",
    "# Group these replay buffers and export as a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    }
   ],
   "source": [
    "# import helper function for getting stock data\n",
    "from getstock import get_stock_data_yf_between_with_indicators\n",
    "# import time library\n",
    "from datetime import datetime, timedelta\n",
    "# get stock data with technical indicators\n",
    "stock_name = 'AAPL'\n",
    "\n",
    "# period of data to get\n",
    "period = 365*6\n",
    "# start_date in format 'YYYY-MM-DD'\n",
    "start_date = '2016-01-01'\n",
    "# calculate end date being x days after start date\n",
    "start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_obj = start_date_obj + timedelta(days=period)\n",
    "end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "interval = '1d'\n",
    "indicators = ['volume_cmf', 'trend_macd', 'momentum_rsi']\n",
    "\n",
    "stockdata = get_stock_data_yf_between_with_indicators(stock_name, start_date, end_date, interval, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  13\n",
      "action shape:  (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# create the gym environment using the stock data\n",
    "import gym\n",
    "from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "init_balance = 10000\n",
    "max_step = len(stockdata)-1\n",
    "\n",
    "env = StockTradingEnv(stockdata, init_balance, max_step, random = False)\n",
    "\n",
    "# check the shape of the state\n",
    "state = env.reset()\n",
    "print('state shape: ', state.shape[0])\n",
    "\n",
    "# investigate the shape of the action\n",
    "action = env.action_space.sample()\n",
    "print('action shape: ', action.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# create dictionary with state, action, reward as keys and store the values in a list\n",
    "# then create a huggingface dataset from the dictionary\n",
    "# then save the huggingface dataset to a file\n",
    "import numpy as np\n",
    "from datasets import Dataset as huggingfaceDataset\n",
    "\n",
    "data = {'data':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mtimestep\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# reset the environment\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdict\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(state\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m timestep \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# create a loop to sample state, action, reward and store in the dictionary\n",
    "num_episodes = 1000\n",
    "for i in range(num_episodes):\n",
    "    # create list for storing state, action, reward\n",
    "    dict = {'state': [], 'action': [], 'reward': [], 'timestep': []}\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    dict['state'].append(state.tolist())\n",
    "    timestep = 0\n",
    "    done = False\n",
    "    # create a loop to sample action, next_state, reward and store in the dictionary\n",
    "    while not done:\n",
    "        # sample action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # store state, action, reward in the dictionary\n",
    "        dict['action'].append(action.tolist())\n",
    "        dict['reward'].append([reward])\n",
    "        dict['timestep'].append(timestep)\n",
    "        # update state\n",
    "        timestep += 1\n",
    "        state = next_state\n",
    "        # check if done\n",
    "        if done:\n",
    "            print('Episode: ', i, 'Timestep: ', timestep)\n",
    "            break\n",
    "        else:\n",
    "            dict['state'].append(state.tolist())\n",
    "    \n",
    "    # store the state, action, reward list in the dictionary\n",
    "    data['data'].append(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = stock_name + '_' + str(period) + '_' + str(start_date) + '_' + str(interval) + '_random_replaybuffer.json'\n",
    "# output the dictionary to a json file\n",
    "import json\n",
    "with open(file_name, 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train an agent using stable baselines\n",
    "# import \n",
    "# import stable baselines\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "from typing import Tuple, Callable\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# create the environment as a vectorized environment for stable baselines training\n",
    "env_stable = DummyVecEnv([lambda: env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Using cuda device\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# create the models\n",
    "modelPPO = PPO(\"MlpPolicy\", env_stable, verbose=1)\n",
    "modelA2C = A2C(\"MlpPolicy\", env_stable, verbose=1)\n",
    "modelDDPG = DDPG(\"MlpPolicy\", env_stable, verbose=1)\n",
    "# store the models' name in a list\n",
    "model_list = [modelPPO, modelA2C, modelDDPG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  (13,)\n",
      "[ 2.67525005e+01  2.67574997e+01  2.62049999e+01  2.63150005e+01\n",
      " -6.01807594e-01  0.00000000e+00  1.00000000e+02  1.00000000e+04\n",
      "  1.00000000e+04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print('state shape: ', state.shape)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape:  (1, 13)\n",
      "[[ 2.675e+01  2.675e+01  2.620e+01  2.631e+01 -6.021e-01  0.000e+00\n",
      "   1.000e+02  1.000e+04  1.000e+04  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# investigate the shape of the state\n",
    "state = env_stable.reset()\n",
    "print('state shape: ', state.shape)\n",
    "print(state)\n",
    "\n",
    "# to-do: find out what happened when dummyvecenv is used and why it caused inf in the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action shape:  (2,)\n",
      "model action shape:  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# investigate the shape of the action\n",
    "action = env.action_space.sample()\n",
    "print('action shape: ', action.shape)\n",
    "model_action, _state = modelPPO.predict(state, deterministic=True)\n",
    "print('model action shape: ', model_action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01672862 0.00804171]]\n",
      "next state shape:  [[ 2.467e+01  2.503e+01  2.411e+01  2.411e+01 -3.411e-01 -2.749e-01\n",
      "   8.511e-01  9.688e+03  1.000e+04  1.200e+01  2.588e+01  0.000e+00\n",
      "   0.000e+00]]\n",
      "reward:  [25.684519]\n",
      "done:  [False]\n",
      "info:  [{}]\n"
     ]
    }
   ],
   "source": [
    "print(model_action)\n",
    "next_state, reward, done, info = env_stable.step(model_action)\n",
    "print('next state shape: ', next_state)\n",
    "print('reward: ', reward)\n",
    "print('done: ', done)\n",
    "print('info: ', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  <stable_baselines3.ppo.ppo.PPO object at 0x7f141eba3df0>\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 981  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m model_list:\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining model: \u001b[39m\u001b[39m'\u001b[39m, model)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mModel trained\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/curatedataset.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEvaluating model: \u001b[39m\u001b[39m'\u001b[39m, model)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    308\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    309\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    310\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    311\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    312\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    313\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:269\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    271\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:208\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sde:\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mreset_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> 208\u001b[0m values, log_prob, entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mevaluate_actions(rollout_data\u001b[39m.\u001b[39;49mobservations, actions)\n\u001b[1;32m    209\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    210\u001b[0m \u001b[39m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/common/policies.py:707\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    705\u001b[0m     latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor\u001b[39m.\u001b[39mforward_actor(pi_features)\n\u001b[1;32m    706\u001b[0m     latent_vf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor\u001b[39m.\u001b[39mforward_critic(vf_features)\n\u001b[0;32m--> 707\u001b[0m distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m    708\u001b[0m log_prob \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    709\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/common/policies.py:664\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    661\u001b[0m mean_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_net(latent_pi)\n\u001b[1;32m    663\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dist\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_std)\n\u001b[1;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[1;32m    666\u001b[0m     \u001b[39m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m    667\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist\u001b[39m.\u001b[39mproba_distribution(action_logits\u001b[39m=\u001b[39mmean_actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m action_std \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones_like(mean_actions) \u001b[39m*\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 50\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (64, 2)) of distribution Normal(loc: torch.Size([64, 2]), scale: torch.Size([64, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "for model in model_list:\n",
    "    print('Training model: ', model)\n",
    "    model.learn(total_timesteps=1)\n",
    "    print('Model trained')\n",
    "    print('Evaluating model: ', model)\n",
    "    mean_reward, std_reward = evaluate_policy(model, env_stable, n_eval_episodes=10)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models\n",
    "for model in model_list:\n",
    "    mean_reward, std_reward = evaluate_policy(model, env_stable, n_eval_episodes=10)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: create a loop to sample state, action from the each models, reward and store in the dictionary\n",
    "output = []\n",
    "num_episodes = 500\n",
    "# loop through the models\n",
    "for model in model_list:\n",
    "    data2 = {'data':[]}\n",
    "    for i in range(num_episodes):\n",
    "        # create list for storing state, action, reward\n",
    "        dict = {'state': [], 'action': [], 'reward': [], 'timestep': []}\n",
    "        # reset the environment\n",
    "        state = env.reset()\n",
    "        dict['state'].append(state.tolist())\n",
    "        timestep = 0\n",
    "        done = False\n",
    "        # create a loop to sample action, next_state, reward and store in the dictionary\n",
    "        while not done:\n",
    "            # sample action\n",
    "            action, _states = model.predict(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # store state, action, reward in the dictionary\n",
    "            dict['action'].append(action.tolist())\n",
    "            dict['reward'].append([reward])\n",
    "            dict['timestep'].append(timestep)\n",
    "            # update state\n",
    "            timestep += 1\n",
    "            state = next_state\n",
    "            # check if done\n",
    "            if done:\n",
    "                print('Episode: ', i, 'Timestep: ', timestep)\n",
    "                break\n",
    "            else:\n",
    "                dict['state'].append(state.tolist())\n",
    "        \n",
    "        # store the state, action, reward list in the dictionary\n",
    "    data2['data'].append(dict)\n",
    "    output.append(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the output list and save each dictionary to a json file\n",
    "for i in range(len(output)):\n",
    "    file_name = stock_name + '_' + period + '_' + start_date + '_' + interval + '_' + model_list[i].__class__.__name__ + '_replaybuffer.json'\n",
    "    # output the dictionary to a json file\n",
    "    import json\n",
    "    with open(file_name, 'w') as fp:\n",
    "        json.dump(output[i], fp)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "648d1dcbd6682217e1a7c0b1a7c0c54c0b39a029f0fde86a6625ffa444d0c385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
