{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook will use a basic GPT based decision transformer in offline reinforcement learning setting to create bot for trading stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# based on https://github.com/nikhilbarhate99/min-decision-transformer/blob/master/decision_transformer/model.py\n",
    "\n",
    "# define the masked causal attention\n",
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.drop_p = drop_p\n",
    "        # feed forward networks which create the query, key and value\n",
    "        self.Q_net = nn.Linear(h_dim, h_dim)\n",
    "        self.K_net = nn.Linear(h_dim, h_dim)\n",
    "        self.V_net = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "        # feed forward network which projects the attention to the correct dimension\n",
    "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "        # dropout layers\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "        # create the mask\n",
    "        mask = torch.tril(torch.ones(max_T, max_T)).view(1, 1, max_T, max_T)\n",
    "\n",
    "        # register_buffer will make the mask a constant tensor\n",
    "        # so that it will not be included in the model parameters and be updated during backpropagation\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, H]\n",
    "        B, T, C = x.shape # batch size, sequence length, hidden dimension * number of heads\n",
    "        N, D = self.n_heads, C // self.n_heads # number of heads, dimension of each head\n",
    "\n",
    "        # compute the query, key and value\n",
    "        Q = self.Q_net(x).view(B, T, N, D).transpose(1, 2) # [B, N, T, D]\n",
    "        K = self.K_net(x).view(B, T, N, D).transpose(1, 2)\n",
    "        V = self.V_net(x).view(B, T, N, D).transpose(1, 2)\n",
    "\n",
    "        # compute the attention\n",
    "        weights = Q @ K.transpose(2,3) / math.sqrt(D) # QK^T / sqrt(D)\n",
    "        weights = weights.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf')) # mask the future tokens\n",
    "        normalized_weights = F.softmax(weights, dim=-1) # softmax along the last dimension\n",
    "        A = self.att_drop(normalized_weights) # attention with dropout\n",
    "\n",
    "        # compute the output\n",
    "        # gather heads and project to correct dimension\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B, T, N*D)\n",
    "        out = self.proj_drop(self.proj_net(attention))\n",
    "\n",
    "        return out\n",
    "\n",
    "# define the attention block with layer normalization and residual connection as well as the feed forward network\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedAttention(h_dim, max_T, n_heads, drop_p)\n",
    "        self.norm1 = nn.LayerNorm(h_dim)\n",
    "        self.norm2 = nn.LayerNorm(h_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(h_dim, 4*h_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*h_dim, h_dim),\n",
    "            nn.Dropout(drop_p)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, H]\n",
    "        # Attention -> LayerNorm -> Residual -> FFN -> LayerNorm -> Residual\n",
    "        out = self.norm1(x + self.attention(x))\n",
    "        out = self.norm2(out + self.ffn(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "# define the decision transformer\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, n_block, h_dim, context_len, n_heads, drop_p, max_timestep = 4096):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        # transformer blocks\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [AttentionBlock(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_block)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "\n",
    "        # embedding layers\n",
    "        self.state_emb = nn.Linear(state_dim, h_dim)\n",
    "        self.act_emb = nn.Linear(act_dim, h_dim)\n",
    "\n",
    "        # decision transformer blocks\n",
    "        self.blocks = nn.ModuleList([AttentionBlock(h_dim, context_len, n_heads, drop_p) for _ in range(n_block)])\n",
    "\n",
    "        # output layers\n",
    "        self.out = nn.Linear(h_dim, act_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pythonenv1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e4d0e3c477dd2964589f9083fab486290bf200d1da93c2f9ad4e1812a99946a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
