{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# this notebook will use a basic GPT based decision transformer in offline reinforcement learning setting to create bot for trading stock\n",
    "# get cuda device\n",
    "# import libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from cust_transf import DecisionTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-fee53e086325bcc9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 907.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets.load import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# load huggingface dataset from json file in replaybuffer folder\n",
    "foldername = 'replaybuffer'\n",
    "\n",
    "# get filenames in folder\n",
    "import os\n",
    "filenames = os.listdir(foldername)\n",
    "\n",
    "# get full path of files\n",
    "full_filenames = [os.path.join(foldername, filename) for filename in filenames]\n",
    "\n",
    "data = load_dataset(\"json\", data_files = full_filenames[0], field = 'data')\n",
    "testdata = data['train']\n",
    "testdata = testdata.with_format('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(testdata['state'][0]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22524/2668472815.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  for reward in data['reward'].apply(lambda x: discount_cumsum(np.array(x, dtype=np.float32), gamma)/rtg_scale):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# utility function to compute the discounted cumulative sum of a vector\n",
    "def discount_cumsum(x, gamma):\n",
    "    disc_cumsum = np.zeros_like(x)\n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "    return disc_cumsum\n",
    "\n",
    "def compute_rtg(data, gamma, rtg_scale):\n",
    "    # create a new dataframe rtg\n",
    "    rtg = []\n",
    "    for reward in data['reward'].apply(lambda x: discount_cumsum(np.array(x, dtype=np.float32), gamma)/rtg_scale):\n",
    "        rtg.append(reward)\n",
    "    return rtg\n",
    "\n",
    "rtg_scale = 1.0\n",
    "gamma = 0.9\n",
    "\n",
    "rtg = pd.Series(compute_rtg(testdata, gamma, rtg_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.23073303e+02, -2.47859222e+02, -2.45029694e+02, -2.77898550e+00,\n",
       "        2.63569412e+01, -8.05767899e+01, -3.33940643e+02, -3.48788574e+02,\n",
       "       -1.74025497e+02, -2.27482925e+02, -1.83353043e+02,  1.05647766e+02,\n",
       "        2.25484314e+02,  2.49560814e+01, -1.97860165e+01, -4.71386108e+01,\n",
       "       -3.33399086e+01, -8.24027252e+00,  7.17410278e+01,  3.12242775e+01,\n",
       "        1.91399803e+01,  7.54288101e+01,  5.03956795e+01,  8.73550873e+01,\n",
       "        6.93546524e+01,  8.42590714e+01,  1.92525253e+02,  1.91613327e+02,\n",
       "        8.61720810e+01, -6.98128357e+01, -1.54644897e+02, -1.82480423e+02,\n",
       "       -2.22440552e+02, -2.76575958e+02, -2.55479858e+02, -3.46526794e+02,\n",
       "       -6.74338135e+02, -6.39636230e+02, -3.65839600e+02, -5.24914124e+02,\n",
       "       -4.62397034e+02, -5.03820740e+02, -5.36601624e+02, -5.32862793e+02,\n",
       "       -4.80391357e+02, -5.07633881e+02, -5.50826599e+02, -4.81754211e+02,\n",
       "       -4.18256104e+02, -4.43573944e+02, -4.59102264e+02, -4.77941925e+02,\n",
       "       -4.99397339e+02, -5.20242249e+02, -4.51847809e+02, -3.16972565e+02,\n",
       "       -4.38928558e+02, -4.02900635e+02, -9.90687103e+01, -1.55764389e+02,\n",
       "       -4.37088318e+01, -3.08361053e+02, -1.23144623e+02, -8.40998306e+01,\n",
       "       -1.59277664e+02, -3.34558716e+02, -2.08441681e+02, -1.80099457e+02,\n",
       "       -2.01563095e+02, -2.09383789e+02, -1.75343887e+02, -1.11285561e+02,\n",
       "       -1.30169846e+02, -5.57885780e+01,  9.77457962e+01, -1.26719315e+02,\n",
       "       -3.98207512e+01, -1.72200348e+02, -2.56599365e+02, -2.84983917e+02,\n",
       "       -4.47322876e+02, -3.14836884e+02, -1.79485687e+02, -1.04011047e+02,\n",
       "       -2.81272583e+02, -2.64295654e+02, -1.05296722e+02, -4.03153343e+01,\n",
       "       -3.41913071e+01, -2.32439056e+02, -9.69855576e+01, -2.91019459e+01,\n",
       "       -1.10778837e+01, -2.63736084e+02, -2.40617966e+02, -3.33905579e+02,\n",
       "       -3.27545715e+02, -3.28339783e+02, -4.25099030e+02, -5.19774048e+02,\n",
       "       -3.33493073e+02, -5.08788086e+02, -5.07668213e+02, -4.40017639e+02,\n",
       "       -4.69015045e+02, -4.47193329e+02, -4.72859680e+02, -4.36189575e+02,\n",
       "       -3.98868988e+02, -3.28514587e+02, -2.94132843e+02, -4.30191315e+02,\n",
       "       -4.14957123e+02, -2.90458679e+02, -2.63488831e+02, -3.25545990e+02,\n",
       "       -2.96173004e+02, -3.37888733e+02, -2.42307190e+02, -2.48771591e+02,\n",
       "       -2.32374008e+02, -2.38701950e+02, -2.08232056e+02, -1.69435028e+02,\n",
       "       -1.20394684e+02, -1.11369347e+02, -1.05286049e+02, -7.91276398e+01,\n",
       "       -4.59732819e+01, -1.11659956e+01,  8.16109772e+01,  1.71447586e+02,\n",
       "       -7.43123245e+01, -2.70295013e+02, -2.63337128e+02, -3.37528015e+02,\n",
       "       -4.51243195e+02, -5.39883972e+02, -6.61132751e+02, -6.51956848e+02,\n",
       "       -4.85827698e+02, -5.03808350e+02, -5.01542389e+02, -4.59324097e+02,\n",
       "       -4.98909637e+02, -4.81322784e+02, -4.34316193e+02, -4.28836365e+02,\n",
       "       -3.89857727e+02, -3.29245117e+02, -3.45709473e+02, -3.46191376e+02,\n",
       "       -3.57171509e+02, -4.32065552e+02, -3.43610260e+02, -3.76714752e+02,\n",
       "       -3.84464966e+02, -4.34626556e+02, -5.09684601e+02, -5.01093384e+02,\n",
       "       -4.16548065e+02, -2.59645569e+02,  2.42500076e+02,  4.93074646e+02,\n",
       "        5.48432800e+02,  6.61379852e+01, -9.31609802e+01, -9.54361725e+01,\n",
       "       -6.52673950e+01,  5.34884567e+01, -2.70706085e+02, -1.16982422e+02,\n",
       "       -1.40071201e+01, -2.22650665e+02, -6.04856253e+00, -2.57179321e+02,\n",
       "       -3.24293945e+02, -4.32448822e+02, -5.40467773e+02, -5.81621521e+02,\n",
       "       -3.81087830e+02, -3.48118988e+02, -5.21288025e+02, -3.77704590e+02,\n",
       "       -4.68862213e+02, -5.49230713e+02, -5.53805908e+02, -3.85773834e+02,\n",
       "       -6.96703873e+01, -1.66623947e+02, -8.41059036e+01, -1.40042358e+02,\n",
       "       -1.63356750e+02, -1.53651001e+02, -1.36657761e+02, -1.16966522e+02,\n",
       "       -1.02862755e+02, -4.95816574e+01, -3.62177277e+01,  9.28372192e+01,\n",
       "       -1.79573936e+01,  8.66582794e+01,  6.36040154e+01, -8.66754532e+01,\n",
       "       -6.48123703e+01, -1.29522186e+02, -2.94780487e+02, -6.85645523e+01,\n",
       "       -9.32070911e-01, -4.19091461e+02, -5.54232605e+02, -4.86121490e+02,\n",
       "       -5.66940063e+02, -6.45439636e+02, -6.45336487e+02, -6.10230103e+02,\n",
       "       -6.47267578e+02, -6.78911743e+02, -4.90900024e+02, -4.72136658e+02,\n",
       "       -4.68155243e+02, -1.19635788e+02, -3.00419647e+02, -2.94330200e+02,\n",
       "       -3.43267944e+02, -3.68914490e+02, -3.84195923e+02, -3.72942078e+02,\n",
       "       -3.97003235e+02, -4.07115204e+02, -4.19275024e+02, -4.33617096e+02,\n",
       "       -3.86916901e+02, -3.28533661e+02, -2.62335388e+02, -5.06428528e+02,\n",
       "       -4.85068512e+02, -5.04403412e+02, -4.38706024e+02, -4.73330444e+02,\n",
       "       -3.91138031e+02, -3.77864075e+02, -3.19084808e+02, -3.50697479e+02,\n",
       "       -2.76194336e+02, -2.64221466e+02, -2.67287292e+02, -2.65105469e+02,\n",
       "       -2.74662445e+02, -2.70061676e+02, -2.60051300e+02, -2.73261505e+02,\n",
       "       -2.61963654e+02, -2.44423691e+02, -2.41669220e+02, -2.18179108e+02,\n",
       "       -2.09249771e+02, -1.96474899e+02, -1.91980164e+02, -3.63175507e+02,\n",
       "       -5.37703247e+02, -2.80710876e+02, -5.25197632e+02, -2.86843872e+02,\n",
       "       -2.38671005e+02, -2.52916077e+02, -1.87316803e+02, -8.79708557e+02,\n",
       "       -6.78629517e+02, -8.02618896e+02,  3.81277885e+01, -2.32315731e+01,\n",
       "       -4.48044678e+02, -3.40793213e+02, -7.28359680e+01, -3.21513855e+02,\n",
       "       -3.66059601e+02, -9.85762691e+00, -1.26001518e+02, -1.10166420e+02,\n",
       "        7.94020367e+00,  5.34111519e+01,  7.08487015e+01, -3.26699905e+01,\n",
       "       -8.68060989e+01,  2.71552391e+01,  7.77532883e+01,  3.31376572e+01,\n",
       "        2.71933350e+02,  1.94533722e+02,  2.09628220e+02,  2.41041473e+02,\n",
       "        3.22007080e+02,  2.12863255e+01,  3.65705452e+01,  2.01147709e+01,\n",
       "       -4.61554909e+01,  4.74850922e+01,  1.32881073e+02,  1.67792969e+02,\n",
       "        2.08252304e+02,  1.38192581e+02,  1.52226379e+02, -5.42156181e+01,\n",
       "       -2.05717056e+02, -1.99295258e+02, -1.95231964e+02, -1.89339706e+02,\n",
       "       -1.91490143e+02, -1.78574249e+02, -1.64172928e+02, -1.47893906e+02,\n",
       "       -1.60773987e+02, -2.77266541e+02, -2.67832428e+02, -2.86649841e+02,\n",
       "       -3.09994843e+02, -2.90355927e+02, -3.02815796e+02, -2.91371857e+02,\n",
       "       -2.94414948e+02, -2.98348022e+02, -3.05156525e+02, -3.13137268e+02,\n",
       "       -3.18461151e+02, -3.20906891e+02, -3.23445007e+02, -3.29504578e+02,\n",
       "       -3.37476288e+02, -3.39737549e+02, -3.46408203e+02, -3.52336090e+02,\n",
       "       -3.58298889e+02, -3.37143951e+02, -3.30586182e+02, -3.32401489e+02,\n",
       "       -3.39164337e+02, -3.57648315e+02, -2.69020020e+02, -1.74609085e+02,\n",
       "       -1.14198494e+02, -1.11585884e+02, -1.96740753e+02, -4.91447525e+01,\n",
       "        2.12353897e+02,  2.42181320e+02, -1.12306023e+02, -6.84295959e+01,\n",
       "       -1.16392670e+02, -8.07148285e+01, -1.26992920e+02, -1.90279388e+02,\n",
       "       -2.76838654e+02, -3.88648193e+02, -4.54121704e+02, -4.74690186e+02,\n",
       "       -4.70390717e+02, -3.31314667e+02, -2.54221191e+02, -2.82890259e+02,\n",
       "       -3.84629944e+02, -4.55229034e+02, -4.66503418e+02, -5.88782166e+02,\n",
       "       -5.14670654e+02, -3.35265015e+02, -3.56793152e+02, -3.39437958e+02,\n",
       "       -3.88660767e+02, -1.96585831e+02, -1.56427322e+02, -2.23621094e+02,\n",
       "       -1.68016632e+02, -2.42060593e+02,  1.61043518e+02,  4.08830200e+02,\n",
       "        2.36142273e+02,  1.61070267e+02,  1.59491844e+01, -1.71655881e+00,\n",
       "        1.67921509e+02,  1.27313416e+02,  1.02781281e+02,  8.60637817e+01,\n",
       "        1.00066193e+02,  1.10225578e+02,  1.83694412e+02,  1.10099503e+02,\n",
       "        1.49631271e+02,  1.26870697e+02,  1.65119354e+02,  2.08103958e+02,\n",
       "        2.70254150e+02,  3.56772705e+02,  4.73951477e+02,  5.79183716e+02,\n",
       "        8.60814362e+01, -6.60460663e+00,  3.34084702e+01, -1.30046326e+02,\n",
       "        4.85847549e+01, -5.73316803e+01, -4.49858589e+01, -1.15134026e+02,\n",
       "       -1.51810684e+02, -1.66756210e+02, -1.11592316e+02, -9.53535385e+01,\n",
       "       -7.89417572e+01, -6.14253426e+01, -4.10042038e+01, -1.82729092e+01,\n",
       "        6.97042513e+00,  3.47305641e+01,  6.62046280e+01,  1.00563332e+02,\n",
       "        1.39337097e+02,  1.82233734e+02,  2.32270981e+02,  2.88435425e+02,\n",
       "        3.55416260e+02,  4.23825531e+02,  4.96367950e+02,  5.74546936e+02,\n",
       "        6.68040710e+02,  7.70019348e+02,  8.92987427e+02,  1.02375146e+03,\n",
       "        8.77019958e+02,  6.89097961e+02,  1.83350830e+02, -3.75657959e+02,\n",
       "       -2.84000275e+02, -1.96170731e+02, -1.88337143e+02, -1.78957748e+02,\n",
       "       -2.42194931e+02, -1.11018211e+02, -1.05796791e+02, -1.02491875e+02,\n",
       "       -1.78041580e+02,  2.57255383e+01, -3.64517479e+01,  6.24772415e+01,\n",
       "        3.38246841e+01,  6.00350227e+01, -3.56867943e+01, -3.65477371e+01,\n",
       "       -3.29552856e+02, -3.75933105e+02, -5.54351379e+02, -5.27473083e+02,\n",
       "       -6.60641846e+02, -6.90746582e+02, -7.24189209e+02, -7.17692505e+02,\n",
       "       -7.57337891e+02, -7.36177246e+02, -9.58664246e+02, -1.13259448e+03,\n",
       "       -7.94246277e+02, -6.71001343e+02, -5.91098206e+02, -5.96692688e+02,\n",
       "       -2.94428772e+02, -3.13241302e+02, -2.43759613e+02, -1.98862015e+02,\n",
       "       -2.56518982e+02, -2.81939117e+02, -2.75291534e+02, -1.45470566e+02,\n",
       "       -1.58168060e+02, -3.07975426e+01, -6.27960320e+01, -1.60755280e+02,\n",
       "       -1.46525803e+02, -1.37042450e+02, -1.91947876e+02, -1.90497879e+02,\n",
       "       -1.15477135e+02, -1.61769440e+02, -1.23681839e+02, -1.04673943e+02,\n",
       "       -6.88570480e+01, -2.74588814e+01, -6.97243023e+00,  3.31410904e+01,\n",
       "        5.66895981e+01,  2.43956699e+01, -1.21582613e+01, -1.00251877e+02,\n",
       "       -6.12076721e+01, -1.08794510e+02, -9.01033401e+01, -8.73826523e+01,\n",
       "       -7.06113205e+01, -5.81775513e+01, -3.87225990e+01, -1.39710751e+01,\n",
       "        2.73391876e+01,  5.75455284e+01,  8.30961380e+01,  1.32358658e+02,\n",
       "        1.82233810e+02,  3.25567139e+02,  3.46593384e+02,  4.35926788e+02,\n",
       "        4.31879883e+02,  3.04223724e+02,  1.96708054e+02,  2.35215073e+02,\n",
       "        2.10595886e+02,  2.97871094e+02,  3.77294128e+02,  3.13179352e+02,\n",
       "        3.82109222e+02,  4.86945007e+02,  5.42615295e+02,  6.24270203e+02,\n",
       "        7.37896423e+02,  6.51471069e+02,  7.50222351e+02,  8.65740906e+02,\n",
       "        1.01215228e+03,  1.19777234e+03,  3.12438629e+02,  4.11816071e+02,\n",
       "        4.12708984e+02,  3.19736542e+02,  2.12386017e+02,  1.89365570e+02,\n",
       "        1.84227982e+02,  2.85653687e+02,  1.66615814e+02, -5.40563889e+01,\n",
       "       -1.03668594e+02, -5.60173187e+01, -8.74334793e+01, -9.01028976e+01,\n",
       "       -7.75643387e+01, -3.66760559e+01, -8.29168034e+00,  2.59188766e+01,\n",
       "        5.83047333e+01, -2.81452789e+02, -1.61126160e+02, -2.53693390e+02,\n",
       "       -1.74129105e+02, -1.84193939e+02, -8.41317291e+01, -4.70410681e+00,\n",
       "       -4.19360199e+01, -1.84372902e+01,  4.87695007e+01, -1.27508545e+02,\n",
       "       -1.39833191e+02, -7.49529409e+00, -2.01332764e+02,  6.88717651e+01,\n",
       "       -1.33955246e+02, -3.09331894e+01,  4.83956909e+01,  5.28662033e+01,\n",
       "       -3.59476257e+02, -4.11451263e+02, -3.89419647e+02, -3.51148102e+02,\n",
       "       -3.58637054e+02, -5.35421082e+02, -4.34130524e+02, -3.85372467e+02,\n",
       "       -3.37720947e+02, -3.09326782e+02, -5.29353981e+01, -4.86857719e+01,\n",
       "        8.18463593e+01,  2.72948322e+01,  1.58741058e+02,  2.92148346e+02,\n",
       "        1.00271233e+02,  1.75010696e+02,  2.81274452e+01, -6.34642410e+01,\n",
       "        7.88562775e+01,  1.11454880e+02,  1.67947861e+02,  1.02854065e+02,\n",
       "        7.89207916e+01,  1.63301178e+02,  2.63510559e+02,  1.36570343e+02,\n",
       "       -1.33529022e+02, -2.61374664e+02, -1.34765488e+02, -2.18801010e+02,\n",
       "       -5.45887878e+02, -5.01619904e+02, -4.79609192e+02, -4.27827332e+02,\n",
       "       -4.62561798e+02, -4.79405365e+02, -5.03166321e+02, -5.15789917e+02,\n",
       "       -5.28636292e+02, -5.50482849e+02, -5.68711914e+02, -5.91001282e+02,\n",
       "       -6.13564270e+02, -6.87749939e+02, -9.89592651e+02, -8.77690125e+02,\n",
       "       -9.94143982e+02, -1.18391211e+03, -1.22265759e+03, -5.19781189e+02,\n",
       "       -2.47407043e+02, -3.26995361e+02, -3.04483612e+02, -2.85875610e+02,\n",
       "       -2.52022781e+02, -2.84189209e+02, -2.66788940e+02, -2.58158691e+02,\n",
       "       -2.44589722e+02, -2.40961472e+02, -2.24144073e+02, -1.96554092e+02,\n",
       "       -1.88788635e+02, -1.60580170e+02, -1.38625244e+02, -1.11927994e+02,\n",
       "       -9.40254364e+01, -4.17375069e+01, -3.04050636e+01, -5.09164810e+01,\n",
       "       -4.70669975e+01, -2.58872375e+01, -1.20818398e+02, -1.13641869e+02,\n",
       "       -9.41723557e+01, -7.25428848e+01, -4.06515503e+01,  6.55815048e+01,\n",
       "        1.22389702e+02, -1.24044876e+01, -3.97509575e+01, -1.00222610e+02,\n",
       "        4.06271179e+02,  6.67614990e+02,  9.10425354e+02,  8.36749878e+02,\n",
       "       -1.95306244e+02,  8.36757355e+01], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(discount_cumsum(testdata['reward'][0], 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-15115.541   , -15115.541   , -15115.541   , -15088.979   ,\n",
       "       -15062.928   , -15112.571   , -15100.548   , -15073.128   ,\n",
       "       -14958.298   , -14957.268   , -14917.811   , -14870.103   ,\n",
       "       -14836.725   , -14827.697   , -14854.829   , -15014.409   ,\n",
       "       -15025.204   , -15005.832   , -14718.554   , -14704.725   ,\n",
       "       -14749.153   , -14685.077   , -14686.39    , -14658.156   ,\n",
       "       -14648.078   , -14627.193   , -14577.926   , -14565.428   ,\n",
       "       -14583.811   , -14570.795   , -14546.372   , -14518.678   ,\n",
       "       -14491.183   , -14463.901   , -14434.848   , -14404.265   ,\n",
       "       -14479.086   , -14389.748   , -14126.493   , -14204.935   ,\n",
       "       -14038.362   , -14048.622   , -14034.304   , -13945.461   ,\n",
       "       -13766.961   , -13747.196   , -13751.504   , -13520.064   ,\n",
       "       -13311.125   , -13454.291   , -13353.268   , -13365.981   ,\n",
       "       -13403.053   , -13418.151   , -13353.193   , -13267.185   ,\n",
       "       -13278.354   , -13245.226   , -13209.216   , -13177.514   ,\n",
       "       -13143.898   , -13114.535   , -13081.396   , -13038.147   ,\n",
       "       -13070.085   , -13158.505   , -13023.636   , -12960.631   ,\n",
       "       -13003.615   , -13014.124   , -12963.461   , -12889.527   ,\n",
       "       -12896.691   , -12816.468   , -12671.228   , -12871.318   ,\n",
       "       -12819.319   , -12839.326   , -12839.546   , -12826.564   ,\n",
       "       -12866.492   , -12803.215   , -12745.395   , -12522.955   ,\n",
       "       -12698.529   , -12652.92    , -12563.94    , -12516.447   ,\n",
       "       -12489.42    , -12503.957   , -12461.247   , -12425.571   ,\n",
       "       -12394.782   , -12375.988   , -12343.008   , -12399.045   ,\n",
       "       -12357.04    , -12323.965   , -12399.8125  , -12461.078   ,\n",
       "       -12191.209   , -12355.974   , -12302.532   , -12159.129   ,\n",
       "       -12146.142   , -12074.8545  , -12053.363   , -11969.909   ,\n",
       "       -11889.797   , -11800.22    , -11742.32    , -11808.618   ,\n",
       "       -11753.883   , -11611.516   , -11559.373   , -11581.277   ,\n",
       "       -11525.131   , -11539.121   , -11404.785   , -11453.14    ,\n",
       "       -11403.548   , -11384.288   , -11351.146   , -11318.337   ,\n",
       "       -11187.42    , -11195.956   , -11216.602   , -11176.435   ,\n",
       "       -11124.265   , -11078.137   , -10901.119   , -10752.918   ,\n",
       "       -10870.891   , -10875.67    , -10843.206   , -10830.067   ,\n",
       "       -10915.787   , -10961.249   , -11099.955   , -10976.322   ,\n",
       "       -10530.448   , -10493.776   , -10407.663   , -10233.309   ,\n",
       "       -10215.601   , -10174.9     , -10133.267   , -10085.161   ,\n",
       "       -10009.639   ,  -9919.691   ,  -9899.61    ,  -9863.739   ,\n",
       "        -9837.993   ,  -9870.742   ,  -9743.317   ,  -9738.481   ,\n",
       "        -9706.616   ,  -9676.099   ,  -9646.166   ,  -9613.446   ,\n",
       "        -9579.481   ,  -9547.684   ,  -9124.427   ,  -8944.892   ,\n",
       "        -8954.653   ,  -9290.153   ,  -9380.894   ,  -9366.408   ,\n",
       "        -9318.78    ,  -9104.94    ,  -9157.257   ,  -9094.84    ,\n",
       "        -9048.082   ,  -9174.07    ,  -9037.304   ,  -9132.059   ,\n",
       "        -9193.499   ,  -9223.26    ,  -9273.25    ,  -9256.346   ,\n",
       "        -9044.352   ,  -8981.514   ,  -9083.51    ,  -8921.482   ,\n",
       "        -8976.776   ,  -9010.333   ,  -8955.846   ,  -8721.471   ,\n",
       "        -8386.099   ,  -8470.708   ,  -8377.044   ,  -8421.826   ,\n",
       "        -8430.162   ,  -8450.675   ,  -8373.835   ,  -8228.035   ,\n",
       "        -8440.819   ,  -8377.544   ,  -8361.203   ,  -8289.088   ,\n",
       "        -8306.871   ,  -8255.88    ,  -8246.957   ,  -8275.685   ,\n",
       "        -8247.363   ,  -8248.279   ,  -8483.491   ,  -8271.6455  ,\n",
       "        -8207.738   ,  -8406.298   ,  -8477.983   ,  -8366.365   ,\n",
       "        -8472.032   ,  -8554.483   ,  -8479.418   ,  -8354.252   ,\n",
       "        -8329.769   ,  -8283.704   ,  -8124.179   ,  -8072.2686  ,\n",
       "        -8029.61    ,  -7884.3047  ,  -7918.5527  ,  -7858.8984  ,\n",
       "        -7896.4004  ,  -7887.618   ,  -7865.7124  ,  -7815.66    ,\n",
       "        -7806.229   ,  -7762.0757  ,  -7718.544   ,  -7720.6743  ,\n",
       "        -7626.2266  ,  -7525.026   ,  -7406.2207  ,  -7694.328   ,\n",
       "        -7602.786   ,  -7569.0483  ,  -7310.478   ,  -7331.8237  ,\n",
       "        -7026.874   ,  -6932.9873  ,  -6720.0347  ,  -6769.18    ,\n",
       "        -6521.5366  ,  -6292.6313  ,  -6364.976   ,  -6328.2197  ,\n",
       "        -6406.9053  ,  -6332.68    ,  -6239.64    ,  -6303.9106  ,\n",
       "        -6205.965   ,  -6076.877   ,  -6052.2935  ,  -5894.8867  ,\n",
       "        -5768.083   ,  -5532.1377  ,  -5530.1807  ,  -5644.069   ,\n",
       "        -5745.4746  ,  -5602.475   ,  -5672.61    ,  -5489.8228  ,\n",
       "        -5432.805   ,  -5415.1187  ,  -5356.7583  ,  -5666.635   ,\n",
       "        -5510.3877  ,  -5520.3145  ,  -5062.8364  ,  -5094.0894  ,\n",
       "        -5344.29    ,  -5241.4663  ,  -5050.271   ,  -5183.8057  ,\n",
       "        -5182.9404  ,  -4904.518   ,  -4976.293   ,  -4948.9556  ,\n",
       "        -4852.86    ,  -4771.4707  ,  -4774.6104  ,  -5009.9165  ,\n",
       "        -5130.2017  ,  -5096.454   ,  -5070.09    ,  -5046.3037  ,\n",
       "        -5016.7896  ,  -4994.0835  ,  -4969.959   ,  -4945.949   ,\n",
       "        -4914.442   ,  -5016.117   ,  -4999.044   ,  -4999.3774  ,\n",
       "        -5005.73    ,  -4966.6147  ,  -4939.8657  ,  -4915.8354  ,\n",
       "        -4891.688   ,  -4870.585   ,  -4847.0527  ,  -4828.795   ,\n",
       "        -4808.576   ,  -4783.7837  ,  -4757.493   ,  -4730.7407  ,\n",
       "        -4731.806   ,  -4685.3667  ,  -4643.3325  ,  -4600.6113  ,\n",
       "        -4591.052   ,  -4577.369   ,  -4547.0933  ,  -4526.499   ,\n",
       "        -4565.765   ,  -4461.077   ,  -4530.584   ,  -4299.189   ,\n",
       "        -4282.6943  ,  -4287.982   ,  -4389.517   ,  -4507.4624  ,\n",
       "        -4482.8975  ,  -4293.0757  ,  -4094.5193  ,  -4025.8025  ,\n",
       "        -4014.2842  ,  -3880.184   ,  -3826.0925  ,  -3743.4475  ,\n",
       "        -3648.8013  ,  -3019.6243  ,  -2716.5444  ,  -2588.5942  ,\n",
       "        -2551.9607  ,  -2666.7456  ,  -2547.8896  ,  -2432.6863  ,\n",
       "        -2357.201   ,  -2336.2715  ,  -2367.1736  ,  -2336.371   ,\n",
       "        -2300.8604  ,  -2274.1282  ,  -2256.2712  ,  -2227.7576  ,\n",
       "        -2203.396   ,  -2174.9666  ,  -2150.4656  ,  -2126.5073  ,\n",
       "        -2103.2915  ,  -2080.7905  ,  -2055.639   ,  -2028.0627  ,\n",
       "        -1999.2588  ,  -1967.8549  ,  -1938.3352  ,  -1911.5021  ,\n",
       "        -1886.1282  ,  -1859.868   ,  -1832.2054  ,  -1805.2288  ,\n",
       "        -1777.3291  ,  -1751.7225  ,  -1727.6133  ,  -1701.311   ,\n",
       "        -1678.6486  ,  -1650.9207  ,  -1625.9004  ,  -1602.174   ,\n",
       "        -1577.0308  ,  -1552.9069  ,  -1525.3354  ,  -1499.1849  ,\n",
       "        -1476.1237  ,  -1452.277   ,  -1428.8452  ,  -1404.4464  ,\n",
       "        -1378.7819  ,  -1354.6495  ,  -1330.379   ,  -1306.4602  ,\n",
       "        -1282.521   ,  -1273.7036  ,  -1215.7086  ,  -1309.0913  ,\n",
       "        -1284.1781  ,  -1324.296   ,  -1299.939   ,  -1279.1277  ,\n",
       "        -1213.2626  ,  -1138.9357  ,  -1029.2498  ,   -961.3045  ,\n",
       "        -1340.2643  ,  -1404.8102  ,  -1371.5624  ,  -1465.1693  ,\n",
       "        -1413.7979  ,  -1403.191   ,  -1379.1826  ,  -1355.0441  ,\n",
       "        -1330.4371  ,  -1330.5048  ,  -1330.5048  ,  -1330.5048  ,\n",
       "        -1330.5048  ,  -1303.1348  ,  -1274.1003  ,  -1244.0663  ,\n",
       "        -1214.3641  ,  -1191.6606  ,  -1150.1179  ,  -1128.4196  ,\n",
       "        -1087.3645  ,  -1052.5323  ,  -1005.98975 ,   -977.9839  ,\n",
       "         -944.99274 ,   -918.5535  ,   -895.8925  ,   -875.8721  ,\n",
       "         -848.7127  ,   -874.40674 ,   -620.85016 ,   -558.64185 ,\n",
       "         -565.79486 ,   -575.8828  ,   -841.1985  ,  -1105.1168  ,\n",
       "        -1028.2242  ,   -890.0929  ,   -862.15283 ,   -833.3836  ,\n",
       "         -895.0446  ,   -718.3389  ,   -691.7321  ,   -664.1255  ,\n",
       "         -705.2254  ,   -534.2411  ,   -576.2111  ,   -505.19626 ,\n",
       "         -515.7327  ,   -491.5432  ,   -531.2151  ,   -516.73346 ,\n",
       "         -649.5128  ,   -641.5195  ,   -697.5106  ,   -663.71326 ,\n",
       "         -679.181   ,   -644.65045 ,   -610.06604 ,   -552.3929  ,\n",
       "         -518.3917  ,   -451.69995 ,   -517.1958  ,   -542.17175 ,\n",
       "         -272.7759  ,   -143.35522 ,    -45.414204], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.load import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utility function to compute the discounted cumulative sum of a vector\n",
    "def discount_cumsum(x, gamma):\n",
    "    disc_cumsum = np.zeros_like(x)\n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "    return disc_cumsum\n",
    "\n",
    "def compute_rtg(data, gamma, rtg_scale):\n",
    "    rtg = []\n",
    "    for reward in data['reward'].apply(lambda x: discount_cumsum(np.array(x, dtype=np.float32), gamma)/rtg_scale):\n",
    "        rtg.append(reward)\n",
    "    return rtg\n",
    "\n",
    "# define a custom dataset class which loads the data, modifies the reward to be the discounted cumulative sum and apply trajectory masking\n",
    "class CustomTrajDataset(Dataset):\n",
    "    def __init__(self, file_name, context_len, gamma, rtg_scale, train=True):\n",
    "        self.gamma = gamma\n",
    "        self.context_len = context_len\n",
    "\n",
    "        # load the data\n",
    "        if train:\n",
    "            self.data = load_dataset(\"json\", data_files = file_name, field = 'data')['train']\n",
    "        else:\n",
    "            self.data = load_dataset(\"json\", data_files = file_name, field = 'data')['test']\n",
    "\n",
    "        self.data = self.data.with_format('pandas')\n",
    "        #min_len = 10**6\n",
    "\n",
    "        # calculate mean and std of states\n",
    "        states = []\n",
    "\n",
    "        for traj in self.data['state']:\n",
    "            states.append(np.array(traj, dtype=np.float32))\n",
    "            #if len(traj) < min_len:\n",
    "            #    min_len = len(traj)\n",
    "        states = np.concatenate(states, axis=0)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "        # calculate rtg\n",
    "        self.rtg = pd.Series(compute_rtg(self.data, gamma, rtg_scale))\n",
    "\n",
    "        # get the len of the dataset\n",
    "        self.stateshape = len(self.data['state'])\n",
    "\n",
    "\n",
    "    def get_state_stats(self):\n",
    "        return self.state_mean, self.state_std        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.stateshape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "\n",
    "            state = np.stack(self.data['state'][idx])\n",
    "            action = np.stack(self.data['action'][idx])\n",
    "            rtg = self.rtg[idx]\n",
    "        except IndexError:\n",
    "            # handle index out of range error\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with length {len(self.data['state'])}\")\n",
    "\n",
    "\n",
    "        data_len = state.shape[0]\n",
    "        \n",
    "        if data_len > self.context_len:\n",
    "            # sample random start index\n",
    "            start_idx = np.random.randint(0, data_len - self.context_len)\n",
    "            # slice the data and convert to torch\n",
    "            state = torch.from_numpy(state[start_idx:start_idx+self.context_len])\n",
    "            action = torch.from_numpy(action[start_idx:start_idx+self.context_len])\n",
    "            rtg = torch.from_numpy(rtg[start_idx:start_idx+self.context_len])\n",
    "            timesteps = torch.arange(start=start_idx, end=start_idx + self.context_len, step=1)\n",
    "            # trajectory mask\n",
    "            mask = torch.ones(self.context_len, dtype=torch.long)\n",
    "        else:\n",
    "            padding_len = self.context_len - data_len\n",
    "\n",
    "            # pad the data with zeros\n",
    "            state = torch.from_numpy(state)\n",
    "            state = torch.cat([state, torch.zeros((padding_len, *state.shape[1:]))], dim=0)\n",
    "\n",
    "            action = torch.from_numpy(action)\n",
    "            action = torch.cat([action, torch.zeros((padding_len, *action.shape[1:]))], dim=0)\n",
    "\n",
    "            rtg = torch.from_numpy(rtg)\n",
    "            rtg = torch.cat([rtg, torch.zeros((padding_len, *rtg.shape[1:]))], dim=0)\n",
    "\n",
    "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
    "\n",
    "            # trajectory mask\n",
    "            mask = torch.cat([torch.ones(data_len, dtype=torch.long), torch.zeros(padding_len, dtype=torch.long)], dim=0)\n",
    "        \n",
    "        return state, action, rtg, timesteps, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-fee53e086325bcc9/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1009.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_2555_2015-01-01_1d_random_replaybuffer.json has 500 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-0d2b6431758d253c/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 931.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_14_1_1d_ppo_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-c931b78b82e2a521/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 985.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_167_2_1d_a2c_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-a7fb97aa0b2d2c17/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 854.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_167_2_1d_ddpg_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-e3558f3f5b1e1a9f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1063.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_167_3_1d_a2c_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-5650ad8da88f517f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1048.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_167_3_1d_ddpg_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-f0f6506387ff2ddb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 691.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_167_3_1d_ppo_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-a3af101adcd158b1/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1076.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_168_4_1d_a2c_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-1391ec1e9d77eb5a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1028.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_168_4_1d_ddpg_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-ad2892aa847ed6ed/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1011.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_168_4_1d_ppo_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-111410c52e68511e/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 944.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_25_2_1d_ppo_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-7494b394211bc8ce/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 979.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_326_1_1d_ddpg_replaybuffer.json has 200 trajectories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/victoru/.cache/huggingface/datasets/json/default-f82cdf55698959cf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1066.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaybuffer/AAPL_test_len_504_1_1d_a2c_replaybuffer.json has 200 trajectories\n",
      "combined_dataset has 2900 trajectories\n"
     ]
    }
   ],
   "source": [
    "# load huggingface dataset from json file in replaybuffer folder\n",
    "foldername = 'replaybuffer'\n",
    "\n",
    "# get filenames in folder\n",
    "import os\n",
    "filenames = os.listdir(foldername)\n",
    "\n",
    "# get full path of files\n",
    "full_filenames = [os.path.join(foldername, filename) for filename in filenames]\n",
    "\n",
    "# create datasets and store in list from the list of filenames \n",
    "context_len = 30\n",
    "Max_balance = 2147483647\n",
    "gamma = 0.99\n",
    "\n",
    "datasets = []\n",
    "testsets = []\n",
    "for name in full_filenames:\n",
    "    dataset = CustomTrajDataset(name, context_len, gamma, Max_balance, train=True)\n",
    "    print(f\"{name} has {len(dataset)} trajectories\")\n",
    "    datasets.append(dataset)\n",
    "    testset = CustomTrajDataset(name, context_len, gamma, Max_balance, train=False)\n",
    "    testsets.append(testset)\n",
    "\n",
    "# concatenate all datasets\n",
    "combined_dataset = torch.utils.data.ConcatDataset(datasets)\n",
    "print(f\"combined_dataset has {len(combined_dataset)} trajectories\")\n",
    "combined_testset = torch.utils.data.ConcatDataset(testsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max reward:  0.014876080683840477\n",
      "min reward:  -4.3969874261631573e-05\n"
     ]
    }
   ],
   "source": [
    "# loop through the dataset and find the highest and lowest reward\n",
    "max_rtg = -math.inf\n",
    "min_rtg = math.inf\n",
    "for dataset in datasets:\n",
    "    for rtg in dataset.rtg:\n",
    "        max_rtg = max(max_rtg, rtg.max())\n",
    "        min_rtg = min(min_rtg, rtg.min())\n",
    "\n",
    "\n",
    "print(\"max reward: \", max_rtg)\n",
    "print(\"min reward: \", min_rtg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "batch_size = 32\n",
    "# small learning rate to try to avoid mixed precision caused NaNs\n",
    "lr = 3e-5\n",
    "wt_decay = 1e-4\n",
    "warmup_steps = 10000\n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(datasets[0], batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, actions, rtg, timestep, traj_mask = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader from the concatenated dataset\n",
    "dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "testloader = DataLoader(combined_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state type:  torch.float64\n",
      "actions type:  torch.float64\n",
      "rtg type:  torch.float64\n",
      "timestep type:  torch.int64\n",
      "traj_mask type:  torch.int64\n"
     ]
    }
   ],
   "source": [
    "# define model parameters\n",
    "# sample 1 batch from dataloader\n",
    "state, actions, rtg, timestep, traj_mask = next(iter(dataloader))\n",
    "# use batch shape to determine state dimension\n",
    "state_dim = state.shape[-1]\n",
    "act_dim = actions.shape[-1] # discrete action space\n",
    "# use batch shape to determine context length\n",
    "\n",
    "# check return tensor type\n",
    "print('state type: ', state.dtype)\n",
    "print('actions type: ', actions.dtype)\n",
    "print('rtg type: ', rtg.dtype)\n",
    "print('timestep type: ', timestep.dtype)\n",
    "print('traj_mask type: ', traj_mask.dtype)\n",
    "\n",
    "\n",
    "n_blocks = 4 # number of transformer blocks\n",
    "h_dim = 96 # hidden dimension\n",
    "n_heads = 8 # number of heads in multi-head attention\n",
    "drop_p = 0.1 # dropout probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = DecisionTransformer(state_dim, act_dim, n_blocks, h_dim, context_len, n_heads, drop_p).to(device)\n",
    "\n",
    "# create optimizer\n",
    "# use larger eps to try to avoid mixed precision overflow caused NaNs\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wt_decay, eps=1e-6)\n",
    "\n",
    "# create scheduler\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1.0, (step + 1) / warmup_steps))\n",
    "\n",
    "# create a GradScaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler(growth_interval=150)\n",
    "min_scale = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 844144\n"
     ]
    }
   ],
   "source": [
    "# get the model parameters size\n",
    "n_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "print(f\"Number of parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m traj_mask \u001b[39m=\u001b[39m traj_mask\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m action_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone(actions)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m return_preds, state_preds, act_preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(norm_state, rtg, timestep, actions)\n\u001b[1;32m     13\u001b[0m \u001b[39m# check shape of norm_state\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape norm_state: \u001b[39m\u001b[39m{\u001b[39;00mnorm_state\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/cust_transf.py:123\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, state, rtg, timestep, actions)\u001b[0m\n\u001b[1;32m    120\u001b[0m time_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_timestep(timestep)\n\u001b[1;32m    122\u001b[0m \u001b[39m# embedding for the state, reward and actions along with time embedding\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m state_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_state(state) \u001b[39m+\u001b[39m time_emb\n\u001b[1;32m    124\u001b[0m rtg_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_rtg(rtg) \u001b[39m+\u001b[39m time_emb\n\u001b[1;32m    125\u001b[0m act_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_act(actions) \u001b[39m+\u001b[39m time_emb\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "# test run the model\n",
    "with torch.no_grad():\n",
    "    state, actions, rtg, timestep , traj_mask= next(iter(dataloader))\n",
    "    state = state.to(device)\n",
    "    actions = actions.to(device)\n",
    "    # convert rtg to float\n",
    "    rtg = rtg.to(device).float()\n",
    "    timestep = timestep.to(device)\n",
    "    traj_mask = traj_mask.to(device)\n",
    "    action_targets = torch.clone(actions).detach().to(device)\n",
    "    return_preds, state_preds, act_preds = model.forward(state, rtg, timestep, actions)\n",
    "\n",
    "    # check shape of state\n",
    "    print(f\"shape state: {state.shape}\")\n",
    "    # check shape of rtg\n",
    "    print(f\"shape rtg: {rtg.shape}\")\n",
    "    # check shape of timestep\n",
    "    print(f\"shape timestep: {timestep.shape}\")\n",
    "    # check shape of actions\n",
    "    print(f\"shape actions: {actions.shape}\")\n",
    "    print(f\"shape act_preds: {act_preds.shape}\")\n",
    "    print(f\"shape action_targets: {action_targets.shape}\")\n",
    "    \n",
    "    # consider only the action that are padded\n",
    "    act_preds = act_preds.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "    action_targets = action_targets.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "\n",
    "    # check shape of action targets\n",
    "    print(action_targets.shape)\n",
    "    # check shape of action predictions\n",
    "    print(act_preds.shape)\n",
    "\n",
    "# check for nan values and inf values in the input and the output of the model\n",
    "print(torch.isnan(state).any())\n",
    "print(torch.isnan(rtg).any())\n",
    "print(torch.isnan(timestep).any())\n",
    "print(torch.isnan(actions).any())\n",
    "print(torch.isnan(act_preds).any())\n",
    "print(torch.isnan(action_targets).any())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# get the start time to calculate training time\n",
    "import datetime\n",
    "\n",
    "# custom training function which take in the model, dataset, optimizer, scheduler, scaler, n_epochs, min_scale\n",
    "def train_model(model, dataloader, optimizer, scheduler, scaler, n_epochs, min_scale):\n",
    "\n",
    "    # record the start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # define training parameters\n",
    "    log_action_losses = []\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for state, actions, rtg, timestep, traj_mask in tqdm(dataloader):\n",
    "            # get batch data to device\n",
    "            state = state.to(device)\n",
    "            actions = actions.to(device)\n",
    "            rtg = rtg.to(device).float()\n",
    "            timestep = timestep.to(device)\n",
    "            traj_mask = traj_mask.to(device)\n",
    "\n",
    "            action_targets = torch.clone(actions).detach().to(device)\n",
    "\n",
    "            # Zeroes out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # run forward pass with autocasting\n",
    "            # disable autocasting for now to avoid mixed precision caused NaNs\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                _, _, act_preds = model.forward(state, rtg, timestep, actions)\n",
    "\n",
    "                # consider only the action that are not padded\n",
    "                act_preds = act_preds.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "                action_targets = action_targets.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "\n",
    "                # calculate losses just for actions\n",
    "                loss = F.mse_loss(act_preds, action_targets, reduction='mean')\n",
    "\n",
    "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # unscale the gradients\n",
    "            scaler.unscale_(optimizer)\n",
    "            # Clips the gradients by norm\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "            # otherwise, optimizer.step() is skipped.\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Updates the learning rate according to the scheduler\n",
    "            scheduler.step()\n",
    "            # Updates the scale for next iteration.\n",
    "            scaler.update()\n",
    "            # enforce min scale to avoid mixed precision caused NaNs\n",
    "            if scaler.get_scale() < min_scale:\n",
    "                scaler._scale = torch.tensor(min_scale).to(scaler._scale)\n",
    "        \n",
    "            # append action loss to log\n",
    "            log_action_losses.append(loss.detach().cpu().item())\n",
    "\n",
    "        # print every 10 loss log\n",
    "        if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "            print(f'Epoch {epoch}: Loss: {log_action_losses[-1]}')\n",
    "\n",
    "    # record the end time\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(f'Training time: {end_time - start_time}')\n",
    "    \n",
    "    return model, log_action_losses\n",
    "\n",
    "# run the model through the test set and evaluate the loss and accuracy\n",
    "def test_eval_model(model, dataloader, num_epochs):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for state, actions, rtg, timestep, traj_mask in tqdm(dataloader):\n",
    "                # get batch data to device\n",
    "                state = state.to(device)\n",
    "                actions = actions.to(device)\n",
    "                rtg = rtg.to(device)\n",
    "                timestep = timestep.to(device)\n",
    "                traj_mask = traj_mask.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast(enabled=False):\n",
    "                    _, _, act_preds = model.forward(state, rtg, timestep, actions)\n",
    "                    # consider only the action that are not padded\n",
    "                    act_preds = act_preds.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "                    action_targets = action_targets.view(-1, act_dim)[traj_mask.view(-1) > 0]\n",
    "\n",
    "                    # calculate losses just for actions\n",
    "                    loss = F.mse_loss(act_preds, action_targets, reduction='mean')\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(dataloader)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f'Epoch {epoch}: Test Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        test_loss += epoch_loss\n",
    "\n",
    "    print(f'Overall Test Loss: {test_loss:.4f}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model loss on test set before training\n",
    "test_epoch = 10\n",
    "test_eval_model(model, testloader, test_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 49.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 0.587468147277832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 67.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 62.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.34it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.32it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.67it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.11it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 60.55it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.85it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.75it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.16it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.60it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.13it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.60it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.52it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.00it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.13it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.38it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.16it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.73it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.36it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.65it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.55it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.43it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.06it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 71.56it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.80it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.45it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 72.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.83it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 71.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.42it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.65it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 61.54it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.06it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.34it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 61.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 60.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.12it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.83it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 70.47it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.85it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.73it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.47it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss: 0.13367286324501038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 67.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.88it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 63.27it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.18it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.11it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.64it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.33it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.09it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.89it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.28it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.69it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.32it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.08it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.73it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.47it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.09it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.83it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.77it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.69it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.21it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.16it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.06it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.51it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.11it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 69.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.52it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.24it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.21it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.36it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 68.04it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.88it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 65.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 64.91it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.71it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.81it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 67.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 66.47it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 71.38it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.08it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.60it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.49it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.65it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.38it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: Loss: 0.2000335305929184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 81.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.08it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.42it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.55it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.71it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.89it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.75it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.36it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.27it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.09it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.56it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.91it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.88it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.11it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.81it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.37it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.05it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.23it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.48it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.15it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.77it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.70it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.50it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.67it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.05it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.11it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.16it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.09it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.06it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.48it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.98it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.78it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 87.70it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.37it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.70it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.21it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.42it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.91it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.67it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.15it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 88.77it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.54it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.07it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.84it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.78it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 89.73it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.56it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 87.98it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.83it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.89it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300: Loss: 0.1423427313566208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 82.54it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.23it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.34it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.23it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 87.00it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.64it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 88.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 87.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 88.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.05it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.86it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 88.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.51it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.88it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.62it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.55it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.22it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 76.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.68it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.99it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 78.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.55it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.86it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.04it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.09it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.54it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.80it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.18it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.21it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.72it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.44it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.19it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.71it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.33it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.08it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.96it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.28it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.38it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.91it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.59it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.87it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.43it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.69it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.52it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.18it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.71it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.58it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.83it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.23it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.38it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: Loss: 0.2025459110736847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:01<00:00, 81.48it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.81it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.64it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.69it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.20it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.76it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.48it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.54it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.13it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.21it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.94it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.35it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.43it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.05it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.10it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.33it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.08it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.41it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.03it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.65it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.37it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.34it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.79it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.24it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.88it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.31it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.85it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.80it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.53it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.46it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.69it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.61it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.43it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.17it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.05it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.66it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.27it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.64it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.60it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 75.51it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.75it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.64it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.39it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 80.14it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.75it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 79.33it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.82it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.92it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.57it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.74it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 81.87it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.65it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.78it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.63it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.34it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.30it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.86it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.95it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.93it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.87it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.26it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 82.90it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.25it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.37it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.97it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.01it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.22it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.43it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.29it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 83.70it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 85.40it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 86.02it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.13it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.42it/s]\n",
      "100%|██████████| 91/91 [00:01<00:00, 84.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499: Loss: 0.19449269771575928\n",
      "Training time: 0:09:55.818934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train model on each dataloader and store log_action_losses in a list\n",
    "_, log_action_losses = train_model(model, dataloader, optimizer, scheduler, scaler, n_epochs, min_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO plot log curve    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model again after training\n",
    "test_eval_model(model, testloader, test_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save model using torch.save() and save it to a directory\n",
    "directory = 'model'\n",
    "model_name = 'AAPL_model.pt'\n",
    "if not os.path.exists(directory):\n",
    "     os.makedirs(directory)\n",
    "torch.save(model.state_dict(), os.path.join(directory, model_name))\n",
    "\n",
    "# write model parameters to a json file\n",
    "\n",
    "model_params = {\n",
    "     'state_dim': state_dim,\n",
    "     'act_dim': act_dim,\n",
    "     'n_blocks': n_blocks,\n",
    "     'h_dim': h_dim,\n",
    "     'context_len': context_len,\n",
    "     'n_heads': n_heads,\n",
    "     'drop_p': drop_p,\n",
    "}\n",
    "with open(os.path.join(directory, 'AAPL_model_params.json'), 'w') as f:\n",
    "     json.dump(model_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('testpython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "648d1dcbd6682217e1a7c0b1a7c0c54c0b39a029f0fde86a6625ffa444d0c385"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
