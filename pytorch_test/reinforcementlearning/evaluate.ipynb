{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by running it on the open ai gym environment\n",
    "# Example of the environment usage:\n",
    "# import gymanisum as gym\n",
    "# import pandas as pd\n",
    "# from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "# load stock price data\n",
    "# df = pd.read_csv('stock_prices.csv')\n",
    "\n",
    "# create trading environment\n",
    "# env = StockTradingEnv(df, init_balance=10000, max_step=1000, random=True)\n",
    "\n",
    "# reset environment to initial state\n",
    "# obs = env.reset()\n",
    "\n",
    "# loop over steps\n",
    "# for i in range(1000):\n",
    "#     # choose random action\n",
    "#     action = env.action_space.sample()\n",
    "#     # step forward in time\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     # render environment\n",
    "#     env.render()\n",
    "#     # check if episode is done\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# the model has four inputs: norm_state, rtg, timestep, actions and three outputs: return_preds, state_preds, act_preds\n",
    "# norm_state is the normalized state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "# rtg is the return to go which is a tensor of shape (batch_size, seq_len)\n",
    "# timestep is the timestep of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# actions is the actions taken by the agent which is a tensor of shape (batch_size, seq_len, act_dim)\n",
    "# return_preds is the predicted return of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# state_preds is the predicted state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "\n",
    "# the custom environment has one input: actions which is a numpy.ndarray with shape (2,) and four outputs: obs, reward, done, info where obs and reward are numpy.ndarray and done and info are bool and dict respectively\n",
    "\n",
    "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep=10, max_test_ep_len=1000, state_mean=None, state_std=None, render=False):\n",
    "    \n",
    "    eval_batch_size = 1 # required for forward pass\n",
    "\n",
    "    results = {}\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    if state_mean is None:\n",
    "        state_mean = torch.zeros(state_dim).to(device)\n",
    "    else:\n",
    "        state_mean = torch.tensor(state_mean).to(device)\n",
    "    \n",
    "    if state_std is None:\n",
    "        state_std = torch.ones(state_dim).to(device)\n",
    "    else:\n",
    "        state_std = torch.tensor(state_std).to(device)\n",
    "\n",
    "    # same as timesteps used for training the transformer\n",
    "    timestep = torch.arange(start = 0, end = max_test_ep_len, step = 1)\n",
    "    timestep = timestep.repeat(eval_batch_size, 1).to(device)\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_eval_ep):\n",
    "\n",
    "            # zeros place holders\n",
    "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim), dtype=torch.float32, device=device)\n",
    "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim), dtype=torch.float32, device=device)\n",
    "            rtg = torch.zeros((eval_batch_size, max_test_ep_len,1), dtype=torch.float32, device=device)\n",
    "\n",
    "            # initialize environment\n",
    "            running_state = env.reset()\n",
    "            running_reward = 0\n",
    "            running_rtg = rtg_target/rtg_scale\n",
    "\n",
    "            for t in range(max_test_ep_len):\n",
    "                total_timesteps += 1\n",
    "                \n",
    "                # add state in placeholder and normalize\n",
    "                states[0,t] = torch.tensor(running_state).to(device)\n",
    "                states[0,t] = (states[0,t] - state_mean)/state_std\n",
    "\n",
    "                # calculate running rtg and add to placeholder\n",
    "                running_rtg = running_rtg - (running_reward/rtg_scale)\n",
    "                rtg[0,t] = running_rtg\n",
    "\n",
    "                if t < context_len:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,:t+1], rtg[:,:t+1], timestep[:,:t+1], actions[:,:t+1])\n",
    "                    act = act_preds[0,t].detach()\n",
    "                else:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,t-context_len+1:t+1], rtg[:,t-context_len+1:t+1], timestep[:,t-context_len+1:t+1], actions[:,t-context_len+1:t+1])\n",
    "                    act = act_preds[0,-1].detach()\n",
    "                \n",
    "                # step in environment using action\n",
    "                running_state, running_reward, done, _ = env.step(act.cpu().numpy())\n",
    "\n",
    "                # add action in placeholder\n",
    "                actions[0,t] = act\n",
    "                total_reward += running_reward\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "    \n",
    "    results['eval/avg_reward'] = total_reward/num_eval_ep\n",
    "    results['eval/avg_steps'] = total_steps/num_eval_ep\n",
    "\n",
    "    return results\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper function for getting stock data\n",
    "from getstock import get_stock_data_yf_between_with_indicators\n",
    "# import time library\n",
    "from datetime import datetime, timedelta\n",
    "# get stock data with technical indicators\n",
    "import json\n",
    "import os\n",
    "\n",
    "stock_name = 'AAPL'\n",
    "output_dir = 'replaybuffer'\n",
    "\n",
    "# period of data to get\n",
    "period = 365*6\n",
    "train_period = 365*3\n",
    "# start_date in format 'YYYY-MM-DD'\n",
    "start_date = '2016-01-01'\n",
    "# calculate end date being x days after start date\n",
    "start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_obj = start_date_obj + timedelta(days=period)\n",
    "end_train_date_obj = start_date_obj + timedelta(days=train_period)\n",
    "end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "interval = '1d'\n",
    "indicators = ['volume_cmf', 'trend_macd', 'momentum_rsi']\n",
    "\n",
    "stockdata = get_stock_data_yf_between_with_indicators(stock_name, start_date, end_date, interval, indicators)\n",
    "\n",
    "# create the test environment\n",
    "from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "init_balance = 10000\n",
    "max_step = len(stockdata)-1\n",
    "\n",
    "env = StockTradingEnv(stockdata, init_balance, max_step, random = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# get directory of model\n",
    "model_dir = 'model'\n",
    "\n",
    "# load the model\n",
    "model = torch.load('model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
