{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from cust_transf import DecisionTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# get directory of model\n",
    "directory = 'model'\n",
    "model_name = 'AAPL_model.pt'\n",
    "model_params = 'AAPL_model_params.json'\n",
    "\n",
    "# load the model parameters\n",
    "with open(os.path.join(directory, model_params)) as json_file:\n",
    "    params = json.load(json_file)\n",
    "\n",
    "# get the parameters\n",
    "state_dim = params['state_dim']\n",
    "act_dim = params['act_dim']\n",
    "n_blocks = params['n_blocks']\n",
    "h_dim = params['h_dim']\n",
    "context_len = params['context_len']\n",
    "n_heads = params['n_heads']\n",
    "drop_p = params['drop_p']\n",
    "\n",
    "model = DecisionTransformer(state_dim, act_dim, n_blocks, h_dim, context_len, n_heads, drop_p).to(device)\n",
    "\n",
    "# load the model in the directory\n",
    "model.load_state_dict(torch.load(os.path.join(directory, model_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# evaluate the model by running it on the open ai gym environment\n",
    "\n",
    "# the model has four inputs: norm_state, rtg, timestep, actions and three outputs: return_preds, state_preds, act_preds\n",
    "# norm_state is the normalized state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "# rtg is the return to go which is a tensor of shape (batch_size, seq_len)\n",
    "# timestep is the timestep of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# actions is the actions taken by the agent which is a tensor of shape (batch_size, seq_len, act_dim)\n",
    "# return_preds is the predicted return of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# state_preds is the predicted state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "\n",
    "# the custom environment has one input: actions which is a numpy.ndarray with shape (2,) and four outputs: obs, reward, done, info where obs and reward are numpy.ndarray and done and info are bool and dict respectively\n",
    "\n",
    "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep=1, max_test_ep_len=1000, state_mean=None, state_std=None, render_mode='None'):\n",
    "    \n",
    "    eval_batch_size = 1 # required for forward pass\n",
    "\n",
    "    results = {}\n",
    "    statistics = {}\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    if state_mean is None:\n",
    "        state_mean = torch.zeros(state_dim).to(device)\n",
    "    else:\n",
    "        state_mean = torch.tensor(state_mean).to(device)\n",
    "    \n",
    "    if state_std is None:\n",
    "        state_std = torch.ones(state_dim).to(device)\n",
    "    else:\n",
    "        state_std = torch.tensor(state_std).to(device)\n",
    "\n",
    "    # same as timesteps used for training the transformer\n",
    "    timestep = torch.arange(start = 0, end = max_test_ep_len, step = 1)\n",
    "    timestep = timestep.repeat(eval_batch_size, 1).to(device)\n",
    "    episode_stats = []\n",
    "    frames = {}\n",
    "\n",
    "    # pick a random episode\n",
    "    rand_ep = np.random.randint(0, num_eval_ep)\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_eval_ep):\n",
    "\n",
    "            # zeros place holders\n",
    "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim), dtype=torch.float32, device=device)\n",
    "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim), dtype=torch.float32, device=device)\n",
    "            rtg = torch.zeros((eval_batch_size, max_test_ep_len,1), dtype=torch.float32, device=device)\n",
    "\n",
    "            # initialize environment\n",
    "            env.reset()\n",
    "            running_state = env.render(mode=None)\n",
    "            running_reward = 0\n",
    "            running_rtg = rtg_target/rtg_scale\n",
    "            done = False\n",
    "            for t in range(max_test_ep_len):\n",
    "                total_steps += 1\n",
    "                \n",
    "                # add state in placeholder and normalize\n",
    "                states[0,t] = torch.tensor(running_state).to(device)\n",
    "                states[0,t] = (states[0,t] - state_mean)/state_std\n",
    "\n",
    "                # calculate running rtg and add to placeholder\n",
    "                running_rtg = running_rtg - (running_reward/rtg_scale)\n",
    "                rtg[0,t] = running_rtg\n",
    "\n",
    "                if t < context_len:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,:t+1], rtg[:,:t+1], timestep[:,:t+1], actions[:,:t+1])\n",
    "                    act = act_preds[0,t].detach()\n",
    "                else:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,t-context_len+1:t+1], rtg[:,t-context_len+1:t+1], timestep[:,t-context_len+1:t+1], actions[:,t-context_len+1:t+1])\n",
    "                    act = act_preds[0,-1].detach()\n",
    "                # check every 50 t\n",
    "                \"\"\"\n",
    "                if t % 20 == 0:\n",
    "                    print('act: ', act)\n",
    "                    print('state: ', states[0,t])\n",
    "                    print('state (to model): ', states[:,:t+1])\n",
    "                    print('act (to model): ', actions[:,:t+1])\n",
    "                \"\"\"\n",
    "                # step in environment using action\n",
    "                _, running_reward, done, _ = env.step(act.cpu().numpy())\n",
    "                running_state = env.render(mode=None)\n",
    "\n",
    "                # add action in placeholder\n",
    "                actions[0,t] = act\n",
    "                total_reward += running_reward\n",
    "                if render_mode == 'Plot' and i == rand_ep:\n",
    "                    fig,step = env.render(mode='plot')\n",
    "                    frames[step] = fig\n",
    "                if done:\n",
    "                    print('Episode finished after {} timesteps'.format(t+1))\n",
    "                    break\n",
    "            if not done:\n",
    "                print('Episode finished after {} timesteps'.format(max_test_ep_len+1))        \n",
    "            end_state = env.render(mode=None)\n",
    "            statistics['end_balance'] = end_state[-6]\n",
    "            statistics['end_networth'] = end_state[-5]\n",
    "            statistics['episode'] = i\n",
    "\n",
    "            # append the stat for this ep to the list\n",
    "            episode_stats.append(statistics.copy())\n",
    "\n",
    "    for key, value in statistics.items():\n",
    "        results['eval/statistics/' + key] = value\n",
    "    \n",
    "    # find the max end_balance and end_networth across all episodes\n",
    "    max_end_balance = max([ep['end_balance'] for ep in episode_stats])\n",
    "    max_end_networth = max([ep['end_networth'] for ep in episode_stats])\n",
    "\n",
    "    results['max_end_balance'] = max_end_balance\n",
    "    results['max_end_networth'] = max_end_networth\n",
    "    results['eval/avg_reward'] = total_reward/num_eval_ep\n",
    "    results['eval/avg_steps'] = total_steps/num_eval_ep\n",
    "    if render_mode == 'Plot':\n",
    "        results['frames'] = frames\n",
    "\n",
    "    return results\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:780: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:785: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n",
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# import helper function for getting stock data\n",
    "from getstock import get_stock_data_yf_between_with_indicators\n",
    "# import time library\n",
    "from datetime import datetime, timedelta\n",
    "# get stock data with technical indicators\n",
    "import json\n",
    "import os\n",
    "\n",
    "stock_name = 'AAPL'\n",
    "\n",
    "# period of data to get\n",
    "period = 365\n",
    "\n",
    "# start_date in format 'YYYY-MM-DD'\n",
    "start_date = '2022-01-01'\n",
    "# calculate end date being x days after start date\n",
    "start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_obj = start_date_obj + timedelta(days=period)\n",
    "\n",
    "end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "interval = '1d'\n",
    "indicators = ['Volume', 'volume_cmf', 'trend_macd', 'momentum_rsi']\n",
    "\n",
    "stockdata = get_stock_data_yf_between_with_indicators(stock_name, start_date, end_date, interval, indicators)\n",
    "\n",
    "# create the test environment\n",
    "from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "init_balance = 10000\n",
    "max_step = len(stockdata)-1\n",
    "\n",
    "env = StockTradingEnv(stockdata, init_balance, max_step, random = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 23 timesteps\n",
      "77\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/StockTradingGraph.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['Volume'] = dfvolume\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Boolean array expected for the condition, not float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m rtg_scale \u001b[39m=\u001b[39m \u001b[39m0.75\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m results \u001b[39m=\u001b[39m evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, max_test_ep_len\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, render_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPlot\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m, in \u001b[0;36mevaluate_on_env\u001b[0;34m(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep, max_test_ep_len, state_mean, state_std, render_mode)\u001b[0m\n\u001b[1;32m     93\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m running_reward\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m render_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPlot\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m i \u001b[39m==\u001b[39m rand_ep:\n\u001b[0;32m---> 95\u001b[0m     fig,step \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender(mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mplot\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     96\u001b[0m     frames[step] \u001b[39m=\u001b[39m fig\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/TradingEnvClass.py:282\u001b[0m, in \u001b[0;36mStockTradingEnv.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualization \u001b[39m=\u001b[39m StockTradingGraph(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdfvolume, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_history, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_worths, windows_size\u001b[39m=\u001b[39mLOOKBACK_WINDOW_SIZE)\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m>\u001b[39m LOOKBACK_WINDOW_SIZE:\n\u001b[0;32m--> 282\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisualization\u001b[39m.\u001b[39;49mplot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_step), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step\n\u001b[1;32m    284\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# return the observation\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_observation()\n",
      "File \u001b[0;32m/media/victoru/B612CEC512CE8A37/ai50/pytorch_test/reinforcementlearning/StockTradingGraph.py:67\u001b[0m, in \u001b[0;36mStockTradingGraph.plot\u001b[0;34m(self, current_step)\u001b[0m\n\u001b[1;32m     63\u001b[0m     sell \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(sell, pad_width\u001b[39m=\u001b[39m((\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(data) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(sell))), mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m, constant_values\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[39m# create a new column for sell marker position (slightly above the high price when the action history indicates sell)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m buy_marker \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mLow\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mwhere(buy)\u001b[39m*\u001b[39m\u001b[39m0.995\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39m# create a new column for buy marker position (slightly below the low price when the action history indicates buy)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m sell_marker \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mHigh\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mwhere(sell)\u001b[39m*\u001b[39m\u001b[39m1.005\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/core/series.py:6131\u001b[0m, in \u001b[0;36mSeries.where\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   6117\u001b[0m \u001b[39m@deprecate_kwarg\u001b[39m(old_arg_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m\"\u001b[39m, new_arg_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   6118\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(\n\u001b[1;32m   6119\u001b[0m     version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcond\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6129\u001b[0m     try_cast: \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m lib\u001b[39m.\u001b[39mNoDefault \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mno_default,\n\u001b[1;32m   6130\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 6131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mwhere(\n\u001b[1;32m   6132\u001b[0m         cond,\n\u001b[1;32m   6133\u001b[0m         other,\n\u001b[1;32m   6134\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   6135\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   6136\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   6137\u001b[0m         try_cast\u001b[39m=\u001b[39;49mtry_cast,\n\u001b[1;32m   6138\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/core/generic.py:9968\u001b[0m, in \u001b[0;36mNDFrame.where\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   9960\u001b[0m \u001b[39mif\u001b[39;00m try_cast \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[1;32m   9961\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   9962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtry_cast keyword is deprecated and will be removed in a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   9963\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfuture version.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   9964\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m   9965\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   9966\u001b[0m     )\n\u001b[0;32m-> 9968\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_where(cond, other, inplace, axis, level)\n",
      "File \u001b[0;32m~/anaconda3/envs/testpython/lib/python3.10/site-packages/pandas/core/generic.py:9653\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m   9650\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(cond, ABCDataFrame):\n\u001b[1;32m   9651\u001b[0m     \u001b[39m# This is a single-dimensional object.\u001b[39;00m\n\u001b[1;32m   9652\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_bool_dtype(cond):\n\u001b[0;32m-> 9653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(dtype\u001b[39m=\u001b[39mcond\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m   9654\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   9655\u001b[0m     \u001b[39mfor\u001b[39;00m dt \u001b[39min\u001b[39;00m cond\u001b[39m.\u001b[39mdtypes:\n",
      "\u001b[0;31mValueError\u001b[0m: Boolean array expected for the condition, not float64"
     ]
    }
   ],
   "source": [
    "rtg_target = 50\n",
    "rtg_scale = 0.75\n",
    "\n",
    "# evaluate the model\n",
    "results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep=3, max_test_ep_len=1000, render_mode='Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval/statistics/end_balance': 24457.239860534668, 'eval/statistics/end_networth': 12403.509803771973, 'eval/statistics/episode': 0, 'max_end_balance': 24457.239860534668, 'max_end_networth': 12403.509803771973, 'eval/avg_reward': -13609.314943154017, 'eval/avg_steps': 251.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
