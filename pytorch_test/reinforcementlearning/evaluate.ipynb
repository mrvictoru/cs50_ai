{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from cust_transf import DecisionTransformer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# get directory of model\n",
    "directory = 'model'\n",
    "model_name = 'AAPL_model.pt'\n",
    "\n",
    "state_dim = 13\n",
    "act_dim = 2 # discrete action space\n",
    "# use batch shape to determine context length\n",
    "\n",
    "context_len = 20\n",
    "n_blocks = 4 # number of transformer blocks\n",
    "h_dim = 96 # hidden dimension\n",
    "n_heads = 8 # number of heads in multi-head attention\n",
    "drop_p = 0.1 # dropout probability\n",
    "\n",
    "model = DecisionTransformer(state_dim, act_dim, n_blocks, h_dim, context_len, n_heads, drop_p).to(device)\n",
    "\n",
    "# load the model in the directory\n",
    "model.load_state_dict(torch.load(os.path.join(directory, model_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model by running it on the open ai gym environment\n",
    "# Example of the environment usage:\n",
    "# import gymanisum as gym\n",
    "# import pandas as pd\n",
    "# from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "# load stock price data\n",
    "# df = pd.read_csv('stock_prices.csv')\n",
    "\n",
    "# create trading environment\n",
    "# env = StockTradingEnv(df, init_balance=10000, max_step=1000, random=True)\n",
    "\n",
    "# reset environment to initial state\n",
    "# obs = env.reset()\n",
    "\n",
    "# loop over steps\n",
    "# for i in range(1000):\n",
    "#     # choose random action\n",
    "#     action = env.action_space.sample()\n",
    "#     # step forward in time\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     # render environment\n",
    "#     env.render()\n",
    "#     # check if episode is done\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# the model has four inputs: norm_state, rtg, timestep, actions and three outputs: return_preds, state_preds, act_preds\n",
    "# norm_state is the normalized state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "# rtg is the return to go which is a tensor of shape (batch_size, seq_len)\n",
    "# timestep is the timestep of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# actions is the actions taken by the agent which is a tensor of shape (batch_size, seq_len, act_dim)\n",
    "# return_preds is the predicted return of the environment which is a tensor of shape (batch_size, seq_len)\n",
    "# state_preds is the predicted state of the environment which is a tensor of shape (batch_size, seq_len, state_dim)\n",
    "\n",
    "# the custom environment has one input: actions which is a numpy.ndarray with shape (2,) and four outputs: obs, reward, done, info where obs and reward are numpy.ndarray and done and info are bool and dict respectively\n",
    "\n",
    "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep=10, max_test_ep_len=1000, state_mean=None, state_std=None, render=False):\n",
    "    \n",
    "    eval_batch_size = 1 # required for forward pass\n",
    "\n",
    "    results = {}\n",
    "    total_reward = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    if state_mean is None:\n",
    "        state_mean = torch.zeros(state_dim).to(device)\n",
    "    else:\n",
    "        state_mean = torch.tensor(state_mean).to(device)\n",
    "    \n",
    "    if state_std is None:\n",
    "        state_std = torch.ones(state_dim).to(device)\n",
    "    else:\n",
    "        state_std = torch.tensor(state_std).to(device)\n",
    "\n",
    "    # same as timesteps used for training the transformer\n",
    "    timestep = torch.arange(start = 0, end = max_test_ep_len, step = 1)\n",
    "    timestep = timestep.repeat(eval_batch_size, 1).to(device)\n",
    "\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_eval_ep):\n",
    "\n",
    "            # zeros place holders\n",
    "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim), dtype=torch.float32, device=device)\n",
    "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim), dtype=torch.float32, device=device)\n",
    "            rtg = torch.zeros((eval_batch_size, max_test_ep_len,1), dtype=torch.float32, device=device)\n",
    "\n",
    "            # initialize environment\n",
    "            running_state = env.reset()\n",
    "            running_reward = 0\n",
    "            running_rtg = rtg_target/rtg_scale\n",
    "\n",
    "            for t in range(max_test_ep_len):\n",
    "                total_steps += 1\n",
    "                \n",
    "                # add state in placeholder and normalize\n",
    "                states[0,t] = torch.tensor(running_state).to(device)\n",
    "                states[0,t] = (states[0,t] - state_mean)/state_std\n",
    "\n",
    "                # calculate running rtg and add to placeholder\n",
    "                running_rtg = running_rtg - (running_reward/rtg_scale)\n",
    "                rtg[0,t] = running_rtg\n",
    "\n",
    "                if t < context_len:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,:t+1], rtg[:,:t+1], timestep[:,:t+1], actions[:,:t+1])\n",
    "                    act = act_preds[0,t].detach()\n",
    "                else:\n",
    "                    # run forward pass to get action\n",
    "                    _, _, act_preds = model.forward(states[:,t-context_len+1:t+1], rtg[:,t-context_len+1:t+1], timestep[:,t-context_len+1:t+1], actions[:,t-context_len+1:t+1])\n",
    "                    act = act_preds[0,-1].detach()\n",
    "                \n",
    "                # step in environment using action\n",
    "                running_state, running_reward, done, _ = env.step(act.cpu().numpy())\n",
    "\n",
    "                # add action in placeholder\n",
    "                actions[0,t] = act\n",
    "                total_reward += running_reward\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "    \n",
    "    results['eval/avg_reward'] = total_reward/num_eval_ep\n",
    "    results['eval/avg_steps'] = total_steps/num_eval_ep\n",
    "\n",
    "    return results\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:780: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/ta/trend.py:785: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n",
      "/home/victoru/anaconda3/envs/testpython/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# import helper function for getting stock data\n",
    "from getstock import get_stock_data_yf_between_with_indicators\n",
    "# import time library\n",
    "from datetime import datetime, timedelta\n",
    "# get stock data with technical indicators\n",
    "import json\n",
    "import os\n",
    "\n",
    "stock_name = 'AAPL'\n",
    "output_dir = 'replaybuffer'\n",
    "\n",
    "# period of data to get\n",
    "period = 365*6\n",
    "train_period = 365*3\n",
    "# start_date in format 'YYYY-MM-DD'\n",
    "start_date = '2016-01-01'\n",
    "# calculate end date being x days after start date\n",
    "start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_obj = start_date_obj + timedelta(days=period)\n",
    "end_train_date_obj = start_date_obj + timedelta(days=train_period)\n",
    "end_date = end_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "interval = '1d'\n",
    "indicators = ['volume_cmf', 'trend_macd', 'momentum_rsi']\n",
    "\n",
    "stockdata = get_stock_data_yf_between_with_indicators(stock_name, start_date, end_date, interval, indicators)\n",
    "\n",
    "# create the test environment\n",
    "from TradingEnvClass import StockTradingEnv\n",
    "\n",
    "init_balance = 10000\n",
    "max_step = len(stockdata)-1\n",
    "\n",
    "env = StockTradingEnv(stockdata, init_balance, max_step, random = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtg_target = 0.001\n",
    "rtg_scale = 1\n",
    "\n",
    "# evaluate the model\n",
    "results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale, num_eval_ep=10, max_test_ep_len=1000, state_mean=None, state_std=None, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval/avg_reward': 3316766.0702451942, 'eval/avg_steps': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
